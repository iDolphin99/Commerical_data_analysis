{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "23d7d438",
   "metadata": {},
   "source": [
    "# 1. Data Load"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f65e8299",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "from scipy import stats #Analysis \n",
    "from scipy.stats import norm # Analysis\n",
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt\n",
    "from matplotlib import rcParams\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9ef4a9e7",
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.font_manager as fm\n",
    "fontlist = fm.findSystemFonts(fontpaths = None, fontext='ttf')\n",
    "\n",
    "from matplotlib import font_manager, rc\n",
    "\n",
    "font_path=\"C:/Windows/Fonts/H2HDRM.TTF\"\n",
    "font=font_manager.FontProperties(fname=font_path).get_name()\n",
    "rc('font',family=font)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "175a42b3",
   "metadata": {},
   "outputs": [],
   "source": [
    "data = pd.read_csv(\"C:/Users/sohee/Desktop/KW/산학연계(졸작)/산학졸작_openUP_Data/kwproja_data_big.csv\",encoding='utf-8')\n",
    "data.head() # original data -> data"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1eecf56c",
   "metadata": {},
   "source": [
    "# EDA \n",
    "\n",
    "2,927,739 rows X 9 columns\n",
    "\n",
    "- 매장 속성 정보\n",
    "  - shop_code -> 식별자 feature, input feature로는 사용하지 않지만 분류를 위해서는 사용할 수 있을 것 같음  \n",
    "  - shop_name -> input feature로는 사용하지 않음 (NLP deep learning 가능성 있음)\n",
    "  - longtitude : 경도, latitude : 위도 -> 매장 위치 (회사 근처, 학교 근처 등 매출 영향성 있음) -> 군집화, labeling 필요\n",
    "  - shop_type_big -> 15 category  -> 업종 (매출 영향성 있음)\n",
    "  - shop_type_small -> 61 category\n",
    "\n",
    "- 매출 정보\n",
    "  - date -> 24 category, 201606~ 201805 까지의 data\n",
    "  - monthly_gain / avearge_sale_price = 한달 총 판매수\n",
    "\n",
    "- 매출 통계 정보-> X\n",
    "\n",
    "\n",
    "##### monthly_gain과 average_sale_price 중 어느 것을 y값으로 둘 것인가? \n",
    "- 월매출 예측 문제로 가정하고 montly_gain 을 y값으로 예측하는 모델 만들기\n",
    "\n",
    "##### shop_code는 input feature에 넣어야 하는가?\n",
    "- 특별한 브랜드가 y값을 결정하는 과적합 요소가 될 수 있으므로\n",
    "- X 에서 shop code, shop name 제외하는 것도 방법\n",
    "- 어느 위치에 어떤 업종으로 어떤 객단가인 매장을 오픈하면 월매출이 어떻게 될까? 문제\n",
    " - X: shop type big, shop type small, longitude, latitude, avg_sale_price, \n",
    " - y: montly_gain"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8160023d",
   "metadata": {},
   "outputs": [],
   "source": [
    "data.columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0e2e2b0d",
   "metadata": {},
   "outputs": [],
   "source": [
    "data.info()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3f6c9722",
   "metadata": {},
   "source": [
    "# 3. Preprocessing\n",
    "- missing value 제거 \n",
    "    - monthly_gain : 3,605 제거 -> 149,790\n",
    "    - shop_type_big : 4,303 제거 -> 145,487 \n",
    "    - gender feature : 1,187 제거 -> 144,300\n",
    "- shop_code, shop_name : 식별자 feature 이므로 drop \n",
    "- date : 아직은 쓸 수 없으므로 drop\n",
    "    - 여기까지 총 144,300 X 27\n",
    "- shop_type_big(13), shop_type_small(367) : label encodding\n",
    "- longitude, latitude : clustering을 통해 labeling 후 해당 두 열은 drop \n",
    "- MinMaxSaclar 정규화 -> 정규화 column의 범위는??"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "746b4c66",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 원본 data와 따로 관리 -> original data = data, input data = input_data \n",
    "# feature drop\n",
    "input_data = data.copy()\n",
    "input_data = input_data.drop(['date', 'shop_code'], axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a9ad35fa",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# There are no missing values \n",
    "# missing value drop - monthly_gain\n",
    "null_index = input_data[input_data['monthly_gain']==0].index \n",
    "print(\"monthly gain null : \", len(null_index))\n",
    "input_data = input_data.drop(null_index)\n",
    "\n",
    "# missing value drop - average_sale_price\n",
    "null_index = input_data[input_data['average_sale_price']==0].index \n",
    "print(\"average sale price null : \", len(null_index))\n",
    "input_data = input_data.drop(null_index)\n",
    "\n",
    "# missing value drop - shop_type_big\n",
    "null_index = input_data[input_data['shop_type_big'].isnull()==True].index\n",
    "print(\"shop type big null : \", len(null_index))\n",
    "print(\"shop type big unique : \", input_data['shop_type_big'].nunique())\n",
    "input_data = input_data.drop(null_index)\n",
    "\n",
    "# missing value drop - shop_type_small \n",
    "null_index = input_data[input_data['shop_type_small'].isnull()==True].index\n",
    "print(\"shop type small null : \", len(null_index))\n",
    "print(\"shop type small unique : \", input_data['shop_type_small'].nunique() )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8c92fcb4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# LabelEncoder\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "\n",
    "# fit transform으로 한번에 처리 가능\n",
    "le = LabelEncoder()\n",
    "input_data['shop_type_big_label'] = le.fit_transform(list(input_data['shop_type_big']))   \n",
    "print(le.classes_)\n",
    "\n",
    "le = LabelEncoder()\n",
    "input_data['shop_type_small_label'] = le.fit_transform(list(input_data['shop_type_small'])) \n",
    "print(le.classes_)\n",
    "\n",
    "# NLP용 preprocessing \n",
    "# shop_name, shop_type_big, shop_type_small = concat_text \n",
    "input_data['concat_text'] = input_data['shop_name'] + \" \" + input_data['shop_type_big']+\" \"+input_data['shop_type_small']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "292714dc",
   "metadata": {},
   "outputs": [],
   "source": [
    "# BinaryEncoder for categorical variable \n",
    "import category_encoders as ce\n",
    "\n",
    "encoder = ce.BinaryEncoder(cols=[\"shop_type_big\", \"shop_type_small\"])\n",
    "df = encoder.fit_transform(input_data[[\"shop_type_big\", \"shop_type_small\"]])\n",
    "\n",
    "input_data = pd.concat([input_data, df], axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "87274362",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Labeling - KMeans Clustering \n",
    "# longitude + latitude = geo \n",
    "from sklearn.cluster import KMeans\n",
    "\n",
    "kmeans = KMeans(n_clusters=9).fit(input_data[['latitude', 'longitude']])\n",
    "print(kmeans.cluster_centers_)\n",
    "print(kmeans.labels_)\n",
    "\n",
    "input_data['geo'] = kmeans.labels_\n",
    "\n",
    "# plotting geo\n",
    "sns.scatterplot(x='longitude' , y='latitude', hue=\"geo\", data=input_data, palette=\"Paired\")\n",
    "plt.title('k-mean')\n",
    "\n",
    "# NLP 처리를 위해서 featrue drop은 생략합니다! \n",
    "#input_data = input_data.drop(['longitude', 'latitude'], axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "26d90712",
   "metadata": {},
   "outputs": [],
   "source": [
    "# average_sale_price -> skewed data \n",
    "# log transfromation \n",
    "from sklearn.preprocessing import MinMaxScaler, RobustScaler, StandardScaler, Normalizer\n",
    "\n",
    "scale_cols = ['average_sale_price']\n",
    "input_data[scale_cols] = input_data[scale_cols].apply(lambda x : np.log1p(x))\n",
    "\n",
    "input_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e977d128",
   "metadata": {},
   "outputs": [],
   "source": [
    "# rcParams['figure.figsize'] = 15,8\n",
    "# sns.boxplot(x='shop_type_big', y='average_sale_price', data=input_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "31684a87",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ax = plt.subplots()\n",
    "# ax = sns.distplot(input_data['monthly_gain'], hist=False)\n",
    "# ax.set_title('Total Gain Density')\n",
    "# ax.set_xlabel('Monthly Gain')\n",
    "# ax.set_ylabel('Unit Probability')\n",
    "# print(input_data['monthly_gain'].value_counts())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "61d58240",
   "metadata": {},
   "source": [
    "# 4. 타겟변수 확인\n",
    "why(or when) to use log transform in ML? \n",
    "- target variable이 non-negative values 일때만 \n",
    "- outlier 값들도 사용해야 하는 경우, outliers that can't be filtered out as they are important to the model.\n",
    "- 현재 주어진 data도 좌측으로 치우쳐진 (right skewed) 형태, 굉장히 극소수의 업종들만이 굉장히 큰 매출을 만들어낼 수 있는 것으로 보임 \n",
    "- 어떤 column, feature가 가장 monthly_gain과 상관관계가 높을까요? \n",
    "- kaggle house price prediction 대회에서도 RMSE가 아닌 RMSLE를 사용함 -> log를 씌운 형태인데 target variable인 집값의 범위가 넒기 때문\n",
    "\n",
    "- Skewness: The longer the right tail, the more positive the tail\n",
    "- Kurtosis (kurtosis / kurtosis): If the kurtosis value (K) is close to 3, the scatter is close to the normal distribution. (K <3), the distributions can be judged to be flattened more smoothly than the normal distribution, and if the kurtosis is a positive number larger than 3 (K> 3), the distribution can be considered to be a more pointed distribution than the normal distribution"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3f828869",
   "metadata": {},
   "outputs": [],
   "source": [
    "# print(\"Skewness: %f\" % input_data['monthly_gain'].skew())\n",
    "# print(\"Kurtosis: %f\" % input_data['monthly_gain'].kurt())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6d0e251d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# plots=pd.DataFrame()\n",
    "# plots['original']=input_data['monthly_gain']\n",
    "# plots['transformed']=np.log1p(input_data['monthly_gain'])\n",
    "# plots['backToOriginal']=np.expm1(np.log1p(input_data['monthly_gain']))\n",
    "\n",
    "# fig, ax = plt.subplots(1,3,figsize=(15,5))\n",
    "# sns.distplot(plots['original'], ax=ax[0]);\n",
    "# sns.distplot(plots['transformed'], ax=ax[1]);\n",
    "# sns.distplot(plots['backToOriginal'], ax=ax[2]);"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "099a7561",
   "metadata": {},
   "outputs": [],
   "source": [
    "# fig = plt.figure(figsize = (15,10))\n",
    "\n",
    "# fig.add_subplot(1,2,1)\n",
    "# res = stats.probplot(input_data['monthly_gain'], plot=plt)\n",
    "\n",
    "# fig.add_subplot(1,2,2)\n",
    "# res = stats.probplot(np.log1p(input_data['monthly_gain']), plot=plt)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "64fff015",
   "metadata": {},
   "source": [
    "# 5. Data split"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "485106f5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# pandas option 설정 하는 코드\n",
    "# monthly_gain의 경우 부동소수점으로 나타나서 보기 어려울땐 윗 줄의 주석을 제거하고 아래에 주석을 추가하고\n",
    "# 다시 원래대로 돌리고 싶다면 아래에 주석제거, 위에 주석추가\n",
    "pd.options.display.float_format = '{:.2f}'.format\n",
    "#pd.reset_option('display.float_format')\n",
    "\n",
    "input_data.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0b37fa3a",
   "metadata": {},
   "outputs": [],
   "source": [
    "group_big_gain = input_data[['shop_type_big_label','monthly_gain']].groupby('shop_type_big_label')\n",
    "df_group_big_gain = group_big_gain.describe().droplevel(axis=1,level=0)\n",
    "\n",
    "# monthly_gain을 shop_type_big 분야별로 정렬하여 출력\n",
    "print(\"내림차순 기준 분야별로 정렬하여 출력합니다 - monthly_gain\")\n",
    "print(\"min: \", list(df_group_big_gain.sort_values(by=['min'], ascending=False).index))\n",
    "print(\"mean:\", list(df_group_big_gain.sort_values(by=['mean'], ascending=False).index))\n",
    "print(\"max: \", list(df_group_big_gain.sort_values(by=['max'], ascending=False).index))\n",
    "\n",
    "df_group_big_gain"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "17d89a01",
   "metadata": {},
   "outputs": [],
   "source": [
    "input_data_y = input_data['monthly_gain'].copy()\n",
    "input_data_X = input_data.drop(['monthly_gain'], axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d50ca414",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split, cross_validate, StratifiedKFold, GridSearchCV\n",
    "\n",
    "# train/ test data 로 split \n",
    "tr_val_X, test_X, tr_val_y, test_y = train_test_split(\n",
    "    input_data_X, \n",
    "    input_data_y, \n",
    "    test_size = 0.2, \n",
    "    random_state = 42,\n",
    "    shuffle=True\n",
    ")\n",
    "\n",
    "# valid/train 로 split\n",
    "train_X, valid_X, train_y, valid_y = train_test_split(\n",
    "    tr_val_X, \n",
    "    tr_val_y, \n",
    "    test_size = 0.2, \n",
    "    random_state = 42,\n",
    "    shuffle=True\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "55412fa2",
   "metadata": {},
   "source": [
    "# 6. Modeling\n",
    "- XGB\n",
    "    - 타겟변수 정규화 실행 \n",
    "- LGBM\n",
    "    - loss parameter : tweedie \n",
    "- Lasso\n",
    "- Ridge"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f42ec416",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split, cross_validate, KFold, TimeSeriesSplit,GridSearchCV\n",
    "from sklearn.metrics import mean_squared_error, r2_score\n",
    "from keras.callbacks import EarlyStopping, ModelCheckpoint\n",
    "\n",
    "from lightgbm import LGBMRegressor\n",
    "from lightgbm import plot_importance \n",
    "from xgboost import XGBRegressor\n",
    "from xgboost import plot_importance\n",
    "from sklearn.linear_model import Ridge"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "abae000a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ml용 data에는 _ml을 붙여줍니다 \n",
    "train_X_ml = train_X.drop(['shop_name', 'shop_type_big', 'shop_type_small', 'longitude', \n",
    "                           'latitude', 'concat_text', 'shop_type_big_label', 'shop_type_small_label'], axis=1).copy()\n",
    "valid_X_ml = valid_X.drop(['shop_name', 'shop_type_big', 'shop_type_small', 'longitude', \n",
    "                           'latitude', 'concat_text', 'shop_type_big_label', 'shop_type_small_label'], axis=1).copy()\n",
    "test_X_ml = test_X.drop(['shop_name', 'shop_type_big', 'shop_type_small', 'longitude',\n",
    "                         'shop_type_big_label', 'shop_type_small_label', 'latitude', 'concat_text'], axis=1).copy()\n",
    "\n",
    "print(len(train_X))\n",
    "print(len(train_X_ml))\n",
    "print(len(valid_X))\n",
    "print(len(valid_X_ml))\n",
    "print(len(test_X))\n",
    "print(len(test_X_ml))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8744eb9a",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_X_ml"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dd0d9a4a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# XGB, LGBM\n",
    "model_xgb = XGBRegressor()\n",
    "#model_lgbm = LGBMRegressor()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0efad09d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# RIdge 모델 (parameter 적용)\n",
    "from sklearn.linear_model import RidgeCV #parameter를 넣어준다는거에서 ridge랑 다름\n",
    "\n",
    "alphas = [0, 0.001, 0.01, 0.1, 1]\n",
    "\n",
    "# RidgeCV는 alpha로 넣고자 하는 값들을 리스트로 전달하면 내부적으로 최적의 alpha값을 찾아냄\n",
    "ridgecv = RidgeCV(alphas=alphas, normalize=True, cv=5)\n",
    "# cv : cross-validation -> 데이터를 k등분한 후 각각에 대하여 검증 진행\n",
    "# 검증 결과 가장 점수가 높은 모델을 채택\n",
    "ridgecv.fit(train_X_ml, train_y)\n",
    "ridgecv_pred = ridgecv.predict(test_X_ml)\n",
    "\n",
    "mae = mean_absolute_error(test_y, ridgecv_pred)\n",
    "r2 = r2_score(test_y, ridgecv_pred)\n",
    "print(f'Test MAE: ${mae:,.0f}')\n",
    "print(f'R2 Score: {r2:,.4f}\\n')\n",
    "\n",
    "print(f'alpha: {ridgecv.alpha_}') # 최종 결정된 alpha값\n",
    "print(f'cv best score: {ridgecv.best_score_}') # 최종 alpha에서의 점수(R^2 of self.predict(X) wrt. y.)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3f5824e5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# #위의 alpha값 넣어준 후 학습 진행하기\n",
    "# model_ridge=Ridge(alpha=0.01)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f1ca1485",
   "metadata": {},
   "outputs": [],
   "source": [
    "# model_ridge.fit(train_X_ml,train_y),eval_set=[(valid_X_ml,valid_y)])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1dee38a6",
   "metadata": {},
   "outputs": [],
   "source": [
    "model_xgb.fit(train_X_ml,train_y,eval_set=[(valid_X_ml,valid_y)])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ddedfe6a",
   "metadata": {},
   "outputs": [],
   "source": [
    "pred_xgb = model_xgb.predict(test_X_ml)\n",
    "pred_xgb\n",
    "#pred_lgbm = model_lgbm.predict(test_X_ml)\n",
    "#pred_ridge=model_ridge.predict(test_X_ml)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "70e4b8f3",
   "metadata": {},
   "source": [
    "## DL, NLP"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c1c0debb",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "from keras.preprocessing.text import Tokenizer\n",
    "\n",
    "tk = Tokenizer()\n",
    "tk.fit_on_texts(input_data['concat_text'])\n",
    "\n",
    "print(list(tk.word_index.items())[:20])\n",
    "print(\"\\nvocab words 개수 : \", len(tk.word_index.items()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b08a78bd",
   "metadata": {},
   "outputs": [],
   "source": [
    "# test for nlp input length \n",
    "# shop_name, shop_type_big, shop_type_small, text_concat 중 가장  긴 input length 찾아야 함 \n",
    "from keras.preprocessing.sequence import pad_sequences\n",
    "\n",
    "seq_data = tk.texts_to_sequences(input_data['concat_text'])\n",
    "print(\"seq_data[0]: \", seq_data[0])\n",
    "\n",
    "pad_seq_data = pad_sequences(seq_data)\n",
    "print(\"pad_seq_data.shpae: \", pad_seq_data.shape)\n",
    "\n",
    "nlp_input_length = pad_seq_data[0].shape[0]\n",
    "print(\"nlp_input_length\", nlp_input_length)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2f6a6c43",
   "metadata": {},
   "outputs": [],
   "source": [
    "#pad_seq_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "be109cd9",
   "metadata": {},
   "outputs": [],
   "source": [
    "def word_embedding(df, nlp_input_length) :\n",
    "    seq_data = tk.texts_to_sequences(df)\n",
    "    pad_seq_data = pad_sequences(seq_data, nlp_input_length)\n",
    "    word_embedding = pad_seq_data\n",
    "    return word_embedding\n",
    "\n",
    "train_X_dl = word_embedding(train_X['concat_text'], nlp_input_length)\n",
    "valid_X_dl = word_embedding(valid_X['concat_text'], nlp_input_length)\n",
    "test_X_dl = word_embedding(test_X['concat_text'], nlp_input_length)\n",
    "\n",
    "print(len(train_X))\n",
    "print(len(train_X_dl))\n",
    "print(len(valid_X))\n",
    "print(len(valid_X_dl))\n",
    "print(len(test_X))\n",
    "print(len(test_X_dl))    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ad8aec96",
   "metadata": {},
   "outputs": [],
   "source": [
    "from keras import Sequential\n",
    "from keras.layers import *\n",
    "import tensorflow as tf\n",
    "import keras\n",
    "from keras.callbacks import EarlyStopping, ModelCheckpoint, ReduceLROnPlateau\n",
    "\n",
    "def create_model(input_dim, output_dim, input_length=nlp_input_length) : \n",
    "    model = Sequential()\n",
    "    # 1 워드 임베딩 학습 \n",
    "    model.add(Embedding(input_dim, output_dim, input_length = nlp_input_length))\n",
    "    \n",
    "    # Classification 학습 \n",
    "    model.add(Flatten())\n",
    "    model.add(Dense(1, activation='relu'))\n",
    "    model.add(Dense(len(set(input_data_y)), activation='linear'))\n",
    "    \n",
    "    model.compile(loss='binary_crossentropy', optimizer='adam', metrics=['accuracy'])\n",
    "    return model\n",
    "\n",
    "\n",
    "input_dim = len(tk.word_index) + 1 # vocab size + padding index =114,537 + 1 = 114,538\n",
    "output_dim = 10\n",
    "\n",
    "model_dl = create_model(input_dim, output_dim)\n",
    "model_dl.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "104a7d0e",
   "metadata": {},
   "outputs": [],
   "source": [
    "hist = model_dl.fit(train_X_dl, train_y, validation_data=(valid_X_dl, valid_y), epochs=10, batch_size=64)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fee9307e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# import matplotlib.pyplot as plt\n",
    "# plt.plot(hist.history['loss'], label='loss')\n",
    "# plt.plot(hist.history['val_loss'], label='test loss')\n",
    "# plt.xticks(range(len(hist.history['loss'])))\n",
    "# plt.xlabel('epoch')\n",
    "# plt.ylabel('precision') \n",
    "# plt.legend(loc='lower right')\n",
    "# plt.grid(True)\n",
    "# plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0ecc1677",
   "metadata": {},
   "outputs": [],
   "source": [
    "pred_dl= model_dl.predict(test_X_dl)\n",
    "pred_dl"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7d1d359c",
   "metadata": {},
   "source": [
    "# Evaluation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9b6c0506",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import accuracy_score, regression_report, confusion_matrix, f1_score\n",
    "\n",
    "def show_pred(test_y, pred) :\n",
    "    true_y = test_y.to_numpy()\n",
    "    true_y = np.ravel(true_y)\n",
    "    \n",
    "    df_result = pd.DataFrame(list(zip(true_y, pred)), columns=['true_y', 'prediction'])\n",
    "    return df_result\n",
    "    \n",
    "def show_prediction_error(test_y, pred) :\n",
    "    true_y = test_y.to_numpy()\n",
    "    true_y = np.ravel(true_y)\n",
    "    error = pred - true_y\n",
    "    plt.hist(error, bins=25)\n",
    "    plt.xlabel(\"Prediction Error\")\n",
    "    _ = plt.ylabel(\"Count\")\n",
    "    \n",
    "def feature_importance(model_xgb) : \n",
    "    %matplotlib inline\n",
    "    plt.rcParams['axes.unicode_minus'] = False\n",
    "    font_path = \"C:/Windows/Fonts/NGULIM.TTF\"\n",
    "    font = fm.FontProperties(fname=font_path).get_name()\n",
    "    rc('font', family=font)\n",
    "\n",
    "    fig, ax = plt.subplots(figsize=(10,12))\n",
    "    plot_importance(model_xgb, ax=ax)\n",
    "    \n",
    "def graph(pred, test_label) :\n",
    "    plt.figure(figsize=(16, 9))\n",
    "    plt.plot(test_label, label = 'actual')\n",
    "    plt.plot(pred, label = 'prediction')\n",
    "    plt.legend()\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "28b29279",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import regression_report\n",
    "print(regression_report(test_y, pred_dl))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bd11588d",
   "metadata": {},
   "outputs": [],
   "source": [
    "#show_pred(test_y, pred_xgb)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d14d900d",
   "metadata": {},
   "outputs": [],
   "source": [
    "#show_pred(test_y, pred_lgbm)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fea0ee8b",
   "metadata": {},
   "outputs": [],
   "source": [
    "#show_pred(test_y, pred_ridge)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "89def36c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# show_mse_rmse(test_y, pred_xgb)\n",
    "# show_r2_score(test_y, pred_xgb, test_X_ml)\n",
    "# show_mae(test_y,pred_xgb)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "41d7fb22",
   "metadata": {},
   "outputs": [],
   "source": [
    "# show_mse_rmse(test_y, pred_lgbm)\n",
    "# show_r2_score(test_y, pred_lgbm, test_X_ml)\n",
    "# show_mae(test_y,pred_lgbm)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "663ee35c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# show_mse_rmse(test_y, pred_ridge)\n",
    "# show_r2_score(test_y, pred_ridge, test_X_ml)\n",
    "# show_mae(test_y,pred_ridge)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "04c13238",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
