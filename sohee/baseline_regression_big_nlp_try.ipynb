{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "23d7d438",
   "metadata": {},
   "source": [
    "# 1. Data Load"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "f65e8299",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import math\n",
    "\n",
    "from scipy import stats #Analysis \n",
    "from scipy.stats import norm \n",
    "\n",
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt\n",
    "from matplotlib import rcParams\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "9ef4a9e7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 한글 폰트가 깨지면 실행하세요! \n",
    "import matplotlib.font_manager as fm\n",
    "fontlist = fm.findSystemFonts(fontpaths = None, fontext='ttf')\n",
    "\n",
    "from matplotlib import font_manager, rc\n",
    "\n",
    "font_path=\"C:/Windows/Fonts/H2HDRM.TTF\"\n",
    "font=font_manager.FontProperties(fname=font_path).get_name()\n",
    "rc('font',family=font)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "175a42b3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# data load\n",
    "data = pd.read_csv(\"C:/Users/sohee/Desktop/KW/산학연계(졸작)/산학졸작_openUP_Data/kwproja_data_big.csv\",encoding='utf-8')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1eecf56c",
   "metadata": {},
   "source": [
    "# EDA \n",
    "\n",
    "2,927,739 rows X 9 columns\n",
    "\n",
    "- 매장 속성 정보\n",
    "  - shop_code -> 식별자 feature, input feature로는 사용하지 않지만 분류를 위해서는 사용할 수 있을 것 같음  \n",
    "  - shop_name -> input feature로는 사용하지 않음 (NLP deep learning 가능성 있음)\n",
    "  - longtitude : 경도, latitude : 위도 -> 매장 위치 (회사 근처, 학교 근처 등 매출 영향성 있음) -> 군집화, labeling 필요\n",
    "  - shop_type_big -> 15 category  -> 업종 (매출 영향성 있음)\n",
    "  - shop_type_small -> 61 category\n",
    "\n",
    "- 매출 정보\n",
    "  - date -> 24 category, 201606~ 201805 까지의 data\n",
    "  - monthly_gain / avearge_sale_price = 한달 총 판매수\n",
    "\n",
    "- 매출 통계 정보-> X\n",
    "\n",
    "\n",
    "##### monthly_gain과 average_sale_price 중 어느 것을 y값으로 둘 것인가? \n",
    "- 월매출 예측 문제로 가정하고 montly_gain 을 y값으로 예측하는 모델 만들기\n",
    "\n",
    "##### shop_code는 input feature에 넣어야 하는가?\n",
    "- 특별한 브랜드가 y값을 결정하는 과적합 요소가 될 수 있으므로\n",
    "- X 에서 shop code, shop name 제외하는 것도 방법\n",
    "- 어느 위치에 어떤 업종으로 어떤 객단가인 매장을 오픈하면 월매출이 어떻게 될까? 문제\n",
    " - X: shop type big, shop type small, longitude, latitude, avg_sale_price, \n",
    " - y: montly_gain"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8160023d",
   "metadata": {},
   "outputs": [],
   "source": [
    "data.columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0e2e2b0d",
   "metadata": {},
   "outputs": [],
   "source": [
    "data.info()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3f6c9722",
   "metadata": {},
   "source": [
    "# 3. Preprocessing\n",
    "data -> processed_data\n",
    "- 9 columns : **shop_code | date | shop_name | shop_type_big | shop_type_small | longitude | latitude | average_sale_price | monthly_gain**\n",
    "- shop_code : 식별자 feature 이므로 drop \n",
    "- date : 아직은 쓸 수 없으므로 drop\n",
    "- missing value 제거 : A/N\n",
    "- shop_type_big(15), shop_type_small(61) \n",
    "    - ML : label encodding\n",
    "    - DL : NLP\n",
    "- longitude, latitude : \n",
    "    - ML : k-mean clustering -> geo column \n",
    "    - DL : NLP, reverse geo encoder(행정동, 법정동, 지번주소, 도로명주소) -> 지번주소 가져오세요(for web) \n",
    "    - 행정동admcode, 법정동legalcode -> area1, area2, area3, area4\n",
    "    - 지번 주소addr -> area1, area2, area3, area4 (x), land -> namber1, number2\n",
    "    - 도로명 주소roadaddr -> area1, area2, area3, area4(x), land -> number1, number2, name  \n",
    "- average_sale_price \n",
    "    - log transformation \n",
    "- MinMaxSaclar 정규화 -> 정규화 column의 범위는?? 실험필요 요인 "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "746b4c66",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 원본 data와 따로 관리 -> original data = data, preprocessed data = processed_data \n",
    "# feature drop : date, shop_code\n",
    "processed_data = data.drop(['date', 'shop_code'], axis=1).copy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a9ad35fa",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# There are no missing values \n",
    "# missing value drop - monthly_gain\n",
    "null_index = processed_data[processed_data['monthly_gain']==0].index \n",
    "print(\"monthly gain null : \", len(null_index))\n",
    "processed_data = processed_data.drop(null_index)\n",
    "\n",
    "# missing value drop - average_sale_price\n",
    "null_index = processed_data[processed_data['average_sale_price']==0].index \n",
    "print(\"average sale price null : \", len(null_index))\n",
    "processed_data = processed_data.drop(null_index)\n",
    "\n",
    "# missing value drop - shop_type_big\n",
    "null_index = processed_data[processed_data['shop_type_big'].isnull()==True].index\n",
    "print(\"shop type big null : \", len(null_index))\n",
    "print(\"shop type big unique : \", processed_data['shop_type_big'].nunique())\n",
    "processed_data = processed_data.drop(null_index)\n",
    "\n",
    "# missing value drop - shop_type_small \n",
    "null_index = processed_data[processed_data['shop_type_small'].isnull()==True].index\n",
    "print(\"shop type small null : \", len(null_index))\n",
    "print(\"shop type small unique : \", processed_data['shop_type_small'].nunique() )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bb83a47b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# LabelEncoder\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "\n",
    "# fit transform으로 한번에 처리 가능\n",
    "le = LabelEncoder()\n",
    "processed_data['shop_type_big_label'] = le.fit_transform(list(processed_data['shop_type_big']))   \n",
    "print(le.classes_)\n",
    "\n",
    "le = LabelEncoder()\n",
    "processed_data['shop_type_small_label'] = le.fit_transform(list(processed_data['shop_type_small'])) \n",
    "print(le.classes_)\n",
    "\n",
    "# NLP용 preprocessing \n",
    "# shop_name, shop_type_big, shop_type_small = concat_text \n",
    "processed_data['concat_text'] = processed_data['shop_name'] + \" \" + processed_data['shop_type_big'] + \" \" + processed_data['shop_type_small']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e08985e0",
   "metadata": {},
   "outputs": [],
   "source": [
    "# BinaryEncoder for categorical variable \n",
    "import category_encoders as ce\n",
    "\n",
    "encoder = ce.BinaryEncoder(cols=[\"shop_type_big\", \"shop_type_small\"])\n",
    "df = encoder.fit_transform(processed_data[[\"shop_type_big\", \"shop_type_small\"]])\n",
    "\n",
    "processed_data = pd.concat([processed_data, df], axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "236323b6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Labeling - KMeans Clustering \n",
    "# longitude + latitude = geo \n",
    "from sklearn.cluster import KMeans\n",
    "\n",
    "kmeans = KMeans(n_clusters=9).fit(processed_data[['latitude', 'longitude']])\n",
    "print(kmeans.cluster_centers_)\n",
    "print(kmeans.labels_)\n",
    "\n",
    "processed_data['geo'] = kmeans.labels_\n",
    "\n",
    "# plotting geo\n",
    "sns.scatterplot(x='longitude' , y='latitude', hue=\"geo\", data=processed_data, palette=\"Paired\")\n",
    "plt.title('k-mean')\n",
    "\n",
    "# NLP 처리를 위해서 featrue drop은 생략합니다! \n",
    "#processed_data = processed_data.drop(['longitude', 'latitude'], axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "de36c722",
   "metadata": {},
   "outputs": [],
   "source": [
    "# average_sale_price -> skewed data \n",
    "# log transfromation \n",
    "from sklearn.preprocessing import MinMaxScaler, RobustScaler, StandardScaler, Normalizer\n",
    "\n",
    "scale_cols = ['average_sale_price']\n",
    "processed_data[scale_cols] = processed_data[scale_cols].apply(lambda x : np.log1p(x))\n",
    "\n",
    "processed_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c2fe3e23",
   "metadata": {},
   "outputs": [],
   "source": [
    "# removing outlier \n",
    "# upper fence, lower fence 외 값(outlier)를 제거합니다 \n",
    "def get_pricelist(i, data) :\n",
    "    df = data[['shop_type_big_label','monthly_gain']].groupby('shop_type_big_label')\n",
    "    Q1 = df.get_group(i)['monthly_gain'].quantile(0.25)\n",
    "    Q2 = df.get_group(i)['monthly_gain'].quantile(0.5)\n",
    "    Q3 = df.get_group(i)['monthly_gain'].quantile(0.75)\n",
    "    IQR = Q3 - Q1\n",
    "    lower_fence = Q1 - (1.5 * IQR)\n",
    "    upper_fence = Q3 + (1.5 * IQR)\n",
    "    if lower_fence <= 0 : lower_fence = 0\n",
    "        \n",
    "    return lower_fence, Q1, Q2, Q3, upper_fence\n",
    "\n",
    "def remove_outlier(data) :\n",
    "    output_data = data.copy()\n",
    "    for i in range(0,15) :\n",
    "        lower_fence, Q1, Q2, Q3, upper_fence = get_pricelist(i, data)\n",
    "        shoptype_index = data[data.shop_type_big_label == i].index\n",
    "        shoptype_data = data.iloc[shoptype_index, :]\n",
    "        outlier_index = shoptype_data[shoptype_data.monthly_gain > upper_fence].index\n",
    "        print(\"removed index in shop_type_big\" , i, \": \", len(outlier_index))\n",
    "        output_data = output_data.drop(outlier_index)\n",
    "    return output_data \n",
    "\n",
    "processed_data = remove_outlier(processed_data)\n",
    "processed_data"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "61d58240",
   "metadata": {},
   "source": [
    "# 4. 타겟변수 확인\n",
    "why(or when) to use log transform in ML? \n",
    "- target variable이 non-negative values 일때만 \n",
    "- outlier 값들도 사용해야 하는 경우, outliers that can't be filtered out as they are important to the model.\n",
    "- 현재 주어진 data도 좌측으로 치우쳐진 (right skewed) 형태, 굉장히 극소수의 업종들만이 굉장히 큰 매출을 만들어낼 수 있는 것으로 보임 \n",
    "- 어떤 column, feature가 가장 monthly_gain과 상관관계가 높을까요? \n",
    "- kaggle house price prediction 대회에서도 RMSE가 아닌 RMSLE를 사용함 -> log를 씌운 형태인데 target variable인 집값의 범위가 넒기 때문\n",
    "\n",
    "- Skewness: The longer the right tail, the more positive the tail\n",
    "- Kurtosis (kurtosis / kurtosis): If the kurtosis value (K) is close to 3, the scatter is close to the normal distribution. (K <3), the distributions can be judged to be flattened more smoothly than the normal distribution, and if the kurtosis is a positive number larger than 3 (K> 3), the distribution can be considered to be a more pointed distribution than the normal distribution"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9306e3c3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# pandas option 설정 하는 코드\n",
    "# monthly_gain의 경우 부동소수점으로 나타나서 보기 어려울땐 윗 줄의 주석을 제거하고 아래에 주석을 추가하고\n",
    "# 다시 원래대로 돌리고 싶다면 아래에 주석제거, 위 코드에 주석추가\n",
    "pd.options.display.float_format = '{:.2f}'.format\n",
    "#pd.reset_option('display.float_format')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "24cef785",
   "metadata": {},
   "source": [
    "# 5. Data Split \n",
    "전처리 완료, 필요한 column을 input으로 넣고 train / valid / test data split "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0eab8ebf",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 전처리된 data와 따로 관리 -> processed_data, model input data = input_data \n",
    "input_data = processed_data.copy()\n",
    "\n",
    "input_data_X = input_data.drop(['monthly_gain'],axis=1)\n",
    "input_data_y = input_data['monthly_gain'].copy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8e8c55ae",
   "metadata": {},
   "outputs": [],
   "source": [
    "input_data_X"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "391ace80",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split, cross_validate, StratifiedKFold, GridSearchCV\n",
    "\n",
    "# train/ test data 로 split \n",
    "tr_val_X, test_X, tr_val_y, test_y = train_test_split(\n",
    "    input_data_X, \n",
    "    input_data_y, \n",
    "    test_size = 0.2, \n",
    "    random_state = 42,\n",
    "    shuffle=True\n",
    ")\n",
    "\n",
    "# valid/train 로 split\n",
    "train_X, valid_X, train_y, valid_y = train_test_split(\n",
    "    tr_val_X, \n",
    "    tr_val_y, \n",
    "    test_size = 0.2, \n",
    "    random_state = 42,\n",
    "    shuffle=True\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "55412fa2",
   "metadata": {},
   "source": [
    "# 6. Modeling"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "f42ec416",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import mean_squared_error, r2_score, mean_absolute_error\n",
    "from keras.callbacks import EarlyStopping, ModelCheckpoint\n",
    "\n",
    "from lightgbm import LGBMRegressor\n",
    "from lightgbm import plot_importance \n",
    "from xgboost import XGBRegressor\n",
    "from xgboost import plot_importance\n",
    "from sklearn.linear_model import Ridge,Lasso"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "b308060a",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# ml용 data에는 _ml을 붙여줍니다 \n",
    "train_X_ml = train_X.drop(['shop_name', 'shop_type_big', 'shop_type_small', 'longitude', \n",
    "                           'latitude', 'concat_text', 'shop_type_big_label', 'shop_type_small_label'], axis=1).copy()\n",
    "valid_X_ml = valid_X.drop(['shop_name', 'shop_type_big', 'shop_type_small', 'longitude', \n",
    "                           'latitude', 'concat_text', 'shop_type_big_label', 'shop_type_small_label'], axis=1).copy()\n",
    "test_X_ml = test_X.drop(['shop_name', 'shop_type_big', 'shop_type_small', 'longitude',\n",
    "                         'shop_type_big_label', 'shop_type_small_label', 'latitude', 'concat_text'], axis=1).copy()\n",
    "\n",
    "#print(len(train_X))\n",
    "#print(len(train_X_ml))\n",
    "#print(len(valid_X))\n",
    "#print(len(valid_X_ml))\n",
    "#print(len(test_X))\n",
    "#print(len(test_X_ml))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3f232825",
   "metadata": {},
   "source": [
    "#### XGB"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d5647721",
   "metadata": {},
   "outputs": [],
   "source": [
    "model_xgb = XGBRegressor()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "77bfb68c",
   "metadata": {},
   "outputs": [],
   "source": [
    "model_xgb.fit(train_X_ml,train_y,eval_set=[(valid_X_ml,valid_y)])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9c237da0",
   "metadata": {},
   "outputs": [],
   "source": [
    "pred_xgb = model_xgb.predict(test_X_ml)\n",
    "pred_xgb"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "075c06ca",
   "metadata": {},
   "source": [
    "#### LGBM"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f72e6a94",
   "metadata": {},
   "outputs": [],
   "source": [
    "model_lgbm = LGBMRegressor()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "484fd0c7",
   "metadata": {},
   "outputs": [],
   "source": [
    "model_lgbm.fit(train_X_ml,train_y,eval_set=[(valid_X_ml,valid_y)])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d47faccb",
   "metadata": {},
   "outputs": [],
   "source": [
    "pred_lgbm = model_lgbm.predict(test_X_ml)\n",
    "pred_lgbm"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b6703f05",
   "metadata": {},
   "source": [
    "#### Ridge"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d0eaa90d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# RIdge 모델 (parameter 적용)\n",
    "from sklearn.linear_model import RidgeCV #parameter를 넣어준다는거에서 ridge랑 다름\n",
    "\n",
    "alphas = [0, 0.001, 0.01, 0.1, 1]\n",
    "\n",
    "# RidgeCV는 alpha로 넣고자 하는 값들을 리스트로 전달하면 내부적으로 최적의 alpha값을 찾아냄\n",
    "ridgecv = RidgeCV(alphas=alphas, normalize=True, cv=5)\n",
    "# cv : cross-validation -> 데이터를 k등분한 후 각각에 대하여 검증 진행\n",
    "# 검증 결과 가장 점수가 높은 모델을 채택\n",
    "ridgecv.fit(train_X_ml, train_y)\n",
    "ridgecv_pred = ridgecv.predict(test_X_ml)\n",
    "\n",
    "mae = mean_absolute_error(test_y, ridgecv_pred)\n",
    "r2 = r2_score(test_y, ridgecv_pred)\n",
    "print(f'Test MAE: ${mae:,.0f}')\n",
    "print(f'R2 Score: {r2:,.4f}\\n')\n",
    "\n",
    "print(f'alpha: {ridgecv.alpha_}') # 최종 결정된 alpha값\n",
    "print(f'cv best score: {ridgecv.best_score_}') # 최종 alpha에서의 점수(R^2 of self.predict(X) wrt. y.)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4e51f2aa",
   "metadata": {},
   "outputs": [],
   "source": [
    "#위의 alpha값 넣어준 후 학습 진행하기\n",
    "model_ridge=Ridge(alpha=0.01)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7e91f4bb",
   "metadata": {},
   "outputs": [],
   "source": [
    "model_ridge.fit(train_X_ml,train_y),eval_set=[(valid_X_ml,valid_y)])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2d361b3f",
   "metadata": {},
   "outputs": [],
   "source": [
    "pred_ridge = model_ridge.predict(test_X_ml)\n",
    "pred_ridge"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "84c35933",
   "metadata": {},
   "source": [
    "#### Lasso"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fec168c7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Parameter 튜닝시도"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ce7fe710",
   "metadata": {},
   "outputs": [],
   "source": [
    "model_lasso=Lasso()\n",
    "model_lasso.fit(train_X_ml,train_y),eval_set=[(valid_X_ml,valid_y)])\n",
    "print(model_lasso.score(train_X_ml,train_y))\n",
    "print(model_lasso.score(test_X_ml,test_y))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dd587d6c",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_score=[]\n",
    "test_score=[]\n",
    "alpha_list=[0.001,0.01,0.1,1,10,100]\n",
    "for alpha in alpha_list:\n",
    "    model_lasso=Lasso(alpha=alpha,max_iter=10000)\n",
    "    model_lasso.fit(train_X_ml,train_y)\n",
    "    train_score.append(model_lasso.score(train_X_ml,train_y))\n",
    "    test_score.append(model_lasso.score(test_X_ml,test_y))\n",
    "plt.plot(np.log10(alpha_list),train_score)\n",
    "plt.plot(np.log10(alpha_list),test_score)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2238f9cf",
   "metadata": {},
   "outputs": [],
   "source": [
    "#test1\n",
    "lasso1= Lasso(alpha=0.01, max_iter=100000).fit(train_X_ml, train_y)\n",
    "\n",
    "print(\"훈련 세트의 정확도 : {:.2f}\".format(lasso1.score(train_X_ml, train_y)))\n",
    "\n",
    "print(\"테스트 세트의 정확도 : {:.2f}\".format(lasso1.score(test_X_ml, test_y)))\n",
    "\n",
    "print(\"사용한 특성의 수 : {}\".format(np.sum(lasso1.coef_ != 0)))\n",
    "\n",
    "print(\"사용한 max_iter : {}\".format(lasso1.n_iter_))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "41e3b410",
   "metadata": {},
   "outputs": [],
   "source": [
    "# --> 차이점이 보이지 않아 default값으로 구현\n",
    "model_lasso=Lasso()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7b7a9ca9",
   "metadata": {},
   "outputs": [],
   "source": [
    "model_lasso.fit(train_X_ml,train_y),eval_set=[(valid_X_ml,valid_y)])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "99a8c881",
   "metadata": {},
   "outputs": [],
   "source": [
    "pred_lasso = model_lasso.predict(test_X_ml)\n",
    "pred_lasso"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fbbc45c6",
   "metadata": {},
   "source": [
    "## 7. NLP"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2eff8d46",
   "metadata": {},
   "outputs": [],
   "source": [
    "from keras.preprocessing.text import Tokenizer\n",
    "\n",
    "tk = Tokenizer()\n",
    "tk.fit_on_texts(input_data['concat_text'])\n",
    "\n",
    "print(list(tk.word_index.items())[:20])\n",
    "print(\"\\nvocab words 개수 : \", len(tk.word_index.items()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bfeed9ef",
   "metadata": {},
   "outputs": [],
   "source": [
    "from keras.preprocessing.sequence import pad_sequences\n",
    "\n",
    "seq_data = tk.texts_to_sequences(input_data['concat_text'])\n",
    "print(\"seq_data[0]: \", seq_data[0])\n",
    "\n",
    "pad_seq_data = pad_sequences(seq_data)\n",
    "print(\"pad_seq_data.shpae: \", pad_seq_data.shape)\n",
    "\n",
    "nlp_input_length = pad_seq_data[0].shape[0]\n",
    "print(\"nlp_input_length\", nlp_input_length)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1432f1a1",
   "metadata": {},
   "outputs": [],
   "source": [
    "def word_embedding(df, nlp_input_length) :\n",
    "    seq_data = tk.texts_to_sequences(df)\n",
    "    pad_seq_data = pad_sequences(seq_data, nlp_input_length)\n",
    "    word_embedding = pad_seq_data\n",
    "    return word_embedding\n",
    "\n",
    "train_X_dl = word_embedding(train_X['concat_text'], nlp_input_length)\n",
    "valid_X_dl = word_embedding(valid_X['concat_text'], nlp_input_length)\n",
    "test_X_dl = word_embedding(test_X['concat_text'], nlp_input_length)\n",
    "\n",
    "print(len(train_X))\n",
    "print(len(train_X_dl))\n",
    "print(len(valid_X))\n",
    "print(len(valid_X_dl))\n",
    "print(len(test_X))\n",
    "print(len(test_X_dl))   "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2598bd79",
   "metadata": {},
   "outputs": [],
   "source": [
    "from keras import Sequential\n",
    "from keras.layers import *\n",
    "import tensorflow as tf\n",
    "import keras\n",
    "from keras.callbacks import EarlyStopping, ModelCheckpoint, ReduceLROnPlateau"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1296cc84",
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_model(input_dim, output_dim, input_length=nlp_input_length) : \n",
    "    model = Sequential()\n",
    "    model.add(Embedding(input_dim, output_dim, input_length = nlp_input_length))    \n",
    "    model.add(Flatten())\n",
    "    model.add(Dense(1, activation='relu'))\n",
    "    model.add(Dense(len(set(input_data_y)), activation='linear'))\n",
    "    \n",
    "    model.compile(loss='binary_crossentropy', optimizer='adam', metrics=['accuracy'])\n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d1f6d6da",
   "metadata": {},
   "outputs": [],
   "source": [
    "input_dim = len(tk.word_index) + 1\n",
    "output_dim = 10\n",
    "\n",
    "model_dl = create_model(input_dim, output_dim)\n",
    "model_dl.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d773a0cd",
   "metadata": {},
   "outputs": [],
   "source": [
    "hist = model_dl.fit(train_X_dl, train_y, validation_data=(valid_X_dl, valid_y), epochs=10, batch_size=64)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9bf6af8a",
   "metadata": {},
   "outputs": [],
   "source": [
    "pred_dl= model_dl.predict(test_X_dl)\n",
    "pred_dl"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c68d1a4e",
   "metadata": {},
   "source": [
    "## Evaluation"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6f9c0398",
   "metadata": {},
   "source": [
    "#### prediction을 확인 후 코드 재작성이 필요"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "063e0b5e",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import accuracy_score, regression_report, confusion_matrix, f1_score\n",
    "\n",
    "def show_pred(test_y, pred) :\n",
    "    true_y = test_y.to_numpy()\n",
    "    true_y = np.ravel(true_y)\n",
    "    \n",
    "    df_result = pd.DataFrame(list(zip(true_y, pred)), columns=['true_y', 'prediction'])\n",
    "    return df_result\n",
    "\n",
    "def show_mse_rmse(test_y, pred) :\n",
    "    mse = mean_squared_error(test_y, pred)\n",
    "    print(\"mse : %f\" % mse)\n",
    "    \n",
    "    rmse = np.sqrt(mse)\n",
    "    print(\"rmse: %f \\n\" %rmse)\n",
    "    \n",
    "def show_mae(test_y,pred):    \n",
    "    mae = mean_absolute_error(test_y, pred)\n",
    "    print(\"mae : %f\" %mae)\n",
    "\n",
    "def show_r2_score(test_y, pred, test_X_ml) : \n",
    "    r2 = r2_score(pred, test_y)\n",
    "    print(\"r2 : %f \" % r2)\n",
    "    adj_r2 = 1 - (1-r2)*(test_X.shape[0]-1)/(test_X_ml.shape[0]-test_X_ml.shape[1]-1)\n",
    "    print(\"adj_r2_score : %f \\n\" % adj_r2)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "572dffd7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# XGB\n",
    "show_pred(test_y, pred_dl)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "91b84402",
   "metadata": {},
   "outputs": [],
   "source": [
    "show_mse_rmse(test_y, pred_dl)\n",
    "show_mae(test_y,pred_dl)\n",
    "show_r2_score(test_y, pred_dl, test_X_ml)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
