{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "e40dea94",
   "metadata": {},
   "source": [
    "# 1. Data Load\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cabb55fa",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import math\n",
    "\n",
    "from scipy import stats #Analysis \n",
    "from scipy.stats import norm \n",
    "\n",
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt\n",
    "from matplotlib import rcParams\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "175a42b3",
   "metadata": {},
   "outputs": [],
   "source": [
    "path = \"./Dataset/\"\n",
    "\n",
    "data = pd.read_csv(path+\"kwproja_data_location.csv\")\n",
    "#data = pd.read_csv('../input/dolphin-kwproja-bigdata/kwproja_data_big.csv')\n",
    "\n",
    "# original data -> data\n",
    "data"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1eecf56c",
   "metadata": {},
   "source": [
    "# 2. EDA \n",
    "\n",
    "2,927,739 rows X 11 columns\n",
    "\n",
    "- 매장 속성 정보\n",
    "  - shop_code -> 식별자 feature => drop   \n",
    "  - shop_name => DL\n",
    "  - longtitude : 경도, latitude : 위도 -> 매장 위치 (회사 근처, 학교 근처 등 매출 영향성 있음) -> K-mean clustering => geo, ML, DL\n",
    "  - address1, address2 : GeoEncoder를 통해 따로 얻은 행정동, 1(30), 2(436) => DL\n",
    "  - shop_type_big -> 15 category, shop_type_big_label, ML => DL\n",
    "  - shop_type_small -> 61 category, shop_type_small_label, ML => DL \n",
    "\n",
    "- 매출 정보\n",
    "  - date -> 24 category, 201606~ 201805 까지의 data\n",
    "  - monthly_gain / avearge_sale_price = 한달 총 판매수\n",
    "\n",
    "- 매출 통계 정보-> X"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4790f693",
   "metadata": {},
   "outputs": [],
   "source": [
    "# rename data columns and check the data\n",
    "data.columns = ['date', 'shop_code', 'shop_name', 'shop_type_big', 'shop_type_small', \n",
    "                'longitude', 'latitude', 'monthly_gain', 'average_sale_price', 'address1', 'address2']\n",
    "\n",
    "print(data.columns, '\\n')\n",
    "print(data.info())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3f6c9722",
   "metadata": {},
   "source": [
    "# 3. Preprocessing\n",
    "data -> processed_data\n",
    "- 9 columns : **shop_code | date | shop_name | shop_type_big | shop_type_small | longitude | latitude | average_sale_price | monthly_gain**\n",
    "- shop_code : 식별자 feature 이므로 drop \n",
    "- date : 아직은 쓸 수 없으므로 drop\n",
    "- missing value 제거 : A/N\n",
    "- shop_type_big(15), shop_type_small(61) \n",
    "    - ML : label encodding\n",
    "    - DL : NLP\n",
    "- longitude, latitude : \n",
    "    - ML : k-mean clustering -> geo column \n",
    "    - DL : NLP, reverse geo encoder(행정동, 법정동, 지번주소, 도로명주소) -> 지번주소 가져오세요(for web) \n",
    "    - 행정동admcode, 법정동legalcode -> area1, area2, area3, area4\n",
    "    - 지번 주소addr -> area1, area2, area3, area4 (x), land -> namber1, number2\n",
    "    - 도로명 주소roadaddr -> area1, area2, area3, area4(x), land -> number1, number2, name  \n",
    "- average_sale_price \n",
    "    - log transformation \n",
    "- MinMaxSaclar 정규화 -> 정규화 column의 범위는?? 실험필요 요인 "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "746b4c66",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 원본 data와 따로 관리 -> original data = data, preprocessed data = processed_data \n",
    "# feature drop : date, shop_code\n",
    "processed_data = data.drop(['date', 'shop_code'], axis=1).copy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a9ad35fa",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# There are no missing values \n",
    "# missing value drop - monthly_gain\n",
    "null_index = processed_data[processed_data['monthly_gain']==0].index \n",
    "print(\"monthly gain null : \", len(null_index))\n",
    "processed_data = processed_data.drop(null_index)\n",
    "\n",
    "# missing value drop - average_sale_price\n",
    "null_index = processed_data[processed_data['average_sale_price']==0].index \n",
    "print(\"average sale price null : \", len(null_index))\n",
    "processed_data = processed_data.drop(null_index)\n",
    "\n",
    "# missing value drop - shop_type_big\n",
    "null_index = processed_data[processed_data['shop_type_big'].isnull()==True].index\n",
    "print(\"shop type big null : \", len(null_index))\n",
    "print(\"shop type big unique : \", processed_data['shop_type_big'].nunique())\n",
    "processed_data = processed_data.drop(null_index)\n",
    "\n",
    "# missing value drop - shop_type_small \n",
    "null_index = processed_data[processed_data['address1'].isnull()==True].index\n",
    "print(\"shop type small null : \", len(null_index))\n",
    "print(\"shop type small unique : \", processed_data['address1'].nunique() )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8bdb9104",
   "metadata": {},
   "outputs": [],
   "source": [
    "# LabelEncoder\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "\n",
    "# fit transform으로 한번에 처리 가능\n",
    "le = LabelEncoder()\n",
    "processed_data['shop_type_big_label'] = le.fit_transform(list(processed_data['shop_type_big']))   \n",
    "print(le.classes_)\n",
    "\n",
    "le = LabelEncoder()\n",
    "processed_data['shop_type_small_label'] = le.fit_transform(list(processed_data['shop_type_small'])) \n",
    "print(le.classes_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "edef0d86",
   "metadata": {},
   "outputs": [],
   "source": [
    "# BinaryEncoder for categorical variable \n",
    "import category_encoders as ce\n",
    "\n",
    "encoder = ce.BinaryEncoder(cols=[\"shop_type_big\", \"shop_type_small\"])\n",
    "df = encoder.fit_transform(processed_data[[\"shop_type_big\", \"shop_type_small\"]])\n",
    "\n",
    "processed_data = pd.concat([processed_data, df], axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "204e2264",
   "metadata": {},
   "outputs": [],
   "source": [
    "# NLP용 preprocessing \n",
    "# shop_name, shop_type_big, shop_type_small = concat_text \n",
    "processed_data['text'] = processed_data['shop_name'] + ' ' + processed_data['shop_type_big'] + ' ' + processed_data['shop_type_small']\n",
    "processed_data['address'] = processed_data['address1'] + ' ' + processed_data['address2']\n",
    "processed_data['concat_text'] = processed_data['text'] + ' ' + processed_data['address']\n",
    "processed_data = processed_data.drop(['text', 'shop_name', 'shop_type_big', 'shop_type_small', 'address', 'address1', 'address2'], axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "35a8be3a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Labeling - KMeans Clustering \n",
    "# longitude + latitude = geo \n",
    "from sklearn.cluster import KMeans\n",
    "\n",
    "kmeans = KMeans(n_clusters=9).fit(processed_data[['latitude', 'longitude']])\n",
    "print(kmeans.cluster_centers_)\n",
    "print(kmeans.labels_)\n",
    "\n",
    "processed_data['geo'] = kmeans.labels_\n",
    "\n",
    "# plotting geo\n",
    "sns.scatterplot(x='longitude' , y='latitude', hue=\"geo\", data=processed_data, palette=\"Paired\")\n",
    "plt.title('k-mean')\n",
    "\n",
    "processed_data = processed_data.drop(['longitude', 'latitude'], axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3538625e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# average_sale_price -> skewed data \n",
    "# log transfromation \n",
    "from sklearn.preprocessing import MinMaxScaler, RobustScaler, StandardScaler, Normalizer\n",
    "\n",
    "scale_cols = ['average_sale_price']\n",
    "processed_data[scale_cols] = processed_data[scale_cols].apply(lambda x : np.log1p(x))\n",
    "\n",
    "processed_data"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "08ffaff6",
   "metadata": {},
   "source": [
    "# 4. Target Variable Labeling and EDA for variable y\n",
    "processed_data -> labeled_data \n",
    "- 어떻게 라벨링 할 것인가? \n",
    "- 1. Classification : use mean, std -> failed\n",
    "- 2. Classification : Quantile 10%, 20%, 25%, 33% -> label 10, 5, 4, 3 \n",
    "- 3. Classification : Quantile by shop_type_big with lower fence, Q2, upper_fence -> label 31\n",
    "- 3. **Classification : Quantile by shop_type_big with Q1, Q2, Q3 -> label 45** \n",
    "- 4. Classification : price label, min:5, max:181억 -> label 15\n",
    "- 5. Removing Outlier : outler 233,140 -> total data(without outlier) 2,694,599\n",
    "- 6. Rounding data "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6d6947c7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# pandas option 설정 하는 코드\n",
    "# monthly_gain의 경우 부동소수점으로 나타나서 보기 어려울땐 윗 줄의 주석을 제거하고 아래에 주석을 추가하고\n",
    "# 다시 원래대로 돌리고 싶다면 아래에 주석제거, 위 코드에 주석추가\n",
    "pd.options.display.float_format = '{:.2f}'.format\n",
    "#pd.reset_option('display.float_format')\n",
    "\n",
    "# 전처리된 data와 따로 관리 -> preprocessed data = procssed_data, labeled data = labeled_data \n",
    "labeled_data = processed_data.copy()\n",
    "labeled_data"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "98e55267",
   "metadata": {},
   "source": [
    "#### Labeling) shop_type_big에 따라 Q1, Q2, Q3, upper fence, lower fence값 이용  \n",
    "\n",
    "- shop_type_big 당 label 3개 : Total lable 30 + 1(lower_fence=0) = 31 \n",
    "- Q1 ~ 이하 값 : lower fence = 0\n",
    "- Q1 ~ Q3 : Q2, mid\n",
    "- Q3 ~ 이상 값 : upper fence \n",
    "\n",
    "- shop_type_big 당 label 3개 : Total label 45\n",
    "- Q1 이하값 : Q1 \n",
    "- Q1 ~ Q3 : Q2, mid\n",
    "- Q3 이상값 : Q3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e567b3a5",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "def get_pricelist(i, data) :\n",
    "    df = data[['shop_type_big_label','monthly_gain']].groupby('shop_type_big_label')\n",
    "    Q1 = df.get_group(i)['monthly_gain'].quantile(0.25)\n",
    "    Q2 = df.get_group(i)['monthly_gain'].quantile(0.5)\n",
    "    Q3 = df.get_group(i)['monthly_gain'].quantile(0.75)\n",
    "    IQR = Q3 - Q1\n",
    "    lower_fence = Q1 - (1.5 * IQR)\n",
    "    upper_fence = Q3 + (1.5 * IQR)\n",
    "    if lower_fence <= 0 : lower_fence = 0\n",
    "        \n",
    "    return lower_fence, Q1, Q2, Q3, upper_fence\n",
    "\n",
    "def labeling(data) : \n",
    "    for i in range(0,15) : \n",
    "        lower_fence, Q1, Q2, Q3, upper_fence = get_pricelist(i, data)\n",
    "        prices = data.loc[(data.shop_type_big_label == i), 'monthly_gain']\n",
    "        #prices = prices.apply(lambda x: lower_fence if x < Q1 else (upper_fence if x > Q3 else Q2))\n",
    "        prices = prices.apply(lambda x : Q1 if x < Q1 else (Q3 if x > Q3 else Q2))\n",
    "        data.loc[(data.shop_type_big_label == i), 'gain_label'] = prices\n",
    "    return data\n",
    "\n",
    "labeled_data = labeling(labeled_data)\n",
    "print(\"label counts: \", labeled_data['gain_label'].nunique())\n",
    "labeled_data"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1d2e7c3c",
   "metadata": {},
   "source": [
    "#### Option) Removing outlier\n",
    "upper fence, lower fence 외 값(outlier)을 제거합니다 \n",
    "- 2,927,739 x 19   ->   2,694,599 x 19"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9178cbda",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "def get_pricelist(i, data) :\n",
    "    df = data[['shop_type_big_label','monthly_gain']].groupby('shop_type_big_label')\n",
    "    Q1 = df.get_group(i)['monthly_gain'].quantile(0.25)\n",
    "    Q2 = df.get_group(i)['monthly_gain'].quantile(0.5)\n",
    "    Q3 = df.get_group(i)['monthly_gain'].quantile(0.75)\n",
    "    IQR = Q3 - Q1\n",
    "    lower_fence = Q1 - (1.5 * IQR)\n",
    "    upper_fence = Q3 + (1.5 * IQR)\n",
    "    if lower_fence <= 0 : lower_fence = 0\n",
    "        \n",
    "    return lower_fence, Q1, Q2, Q3, upper_fence\n",
    "\n",
    "def remove_outlier(data, processed_data) :\n",
    "    output_data = data.copy()\n",
    "    for i in range(0,15) :\n",
    "        lower_fence, Q1, Q2, Q3, upper_fence = get_pricelist(i, processed_data)\n",
    "        shoptype_index = data[data.shop_type_big_label == i].index\n",
    "        shoptype_data = data.iloc[shoptype_index, :]\n",
    "        outlier_index = shoptype_data[shoptype_data.monthly_gain > upper_fence].index\n",
    "        print(\"removed index in shop_type_big\" , i, \": \", len(outlier_index))\n",
    "        output_data = output_data.drop(outlier_index)\n",
    "    return output_data \n",
    "\n",
    "labeled_data = remove_outlier(labeled_data, processed_data)\n",
    "labeled_data"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a2737e09",
   "metadata": {},
   "source": [
    "#### Option) Rounding data\n",
    "십만원대, 백만원대 아래 가격들은 모두 반올림 하여 비슷한 label 값을 가지는 것들은 통일\n",
    "- label의 개수를 줄임 45 -> 36\n",
    "- 1의 자리~ 10,000의 자리 숫자들은 반올림하에 0으로 처리"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6d9e31eb",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "#print(\"original monthly_gain_label counts : \", labeled_data['gain_label'].nunique())\n",
    "#labeled_data['gain_label'] = labeled_data.gain_label.apply(lambda x : round(x, -5) if x < 10000000 else round(x, -6))\n",
    "\n",
    "#print(\"rounded monthly_gain_label value counts :\", labeled_data['gain_label'].nunique(), \n",
    "#      \"\\n\", labeled_data['gain_label'].value_counts())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2520471a",
   "metadata": {},
   "source": [
    "# 5. Data Split \n",
    "모든 전처리와 y variable labeling이 완료되었다. train / valid / test data로 분할하고 용도에 맞게 ML, DL을 돌리도록 하자. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "216beca9",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split, cross_validate, StratifiedKFold, GridSearchCV\n",
    "\n",
    "# continous 값은 lgbm에서 classification 할 수 없음\n",
    "# conver 'float' to 'int'\n",
    "labeled_data = labeled_data.astype({'gain_label' : 'int'})\n",
    "\n",
    "# labeled된 data와 따로 관리 -> labeled data = labeled_data, model input data = input_data\n",
    "input_data = labeled_data.copy()\n",
    "\n",
    "input_data_y = input_data['gain_label'].copy()\n",
    "input_data_X = input_data.drop(['gain_label', 'monthly_gain'], axis=1)\n",
    "\n",
    "# LabelEncoder\n",
    "# y data를 LabelEncdoer로 한 번 더 labeling 합니다 -> 100만원, 10000만원... = 0, 1, ... \n",
    "# 추후에 DL에서 output을 맞춰주기 위함입니다, 최종 마지막에서 원래 label값(100만원, 1000만원...)으로 되돌립니다\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "\n",
    "le = LabelEncoder()\n",
    "input_data_y = le.fit_transform(list(input_data_y))\n",
    "print(\"original gain_label \\n\", le.classes_, '\\n')\n",
    "print(\"gain_lable label \\n\", set(input_data_y), '\\n')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "25cf1583",
   "metadata": {},
   "outputs": [],
   "source": [
    "# train/ test data 로 split \n",
    "tr_val_X, test_X, tr_val_y, test_y = train_test_split(\n",
    "    input_data_X, \n",
    "    input_data_y, \n",
    "    test_size = 0.2, \n",
    "    random_state = 42,\n",
    "    shuffle=True,\n",
    "    stratify = input_data_y # Classification 중요 option\n",
    ")\n",
    "\n",
    "# valid/train 로 split\n",
    "train_X, valid_X, train_y, valid_y = train_test_split(\n",
    "    tr_val_X, \n",
    "    tr_val_y, \n",
    "    test_size = 0.2, \n",
    "    random_state = 42,\n",
    "    shuffle=True,\n",
    "    stratify = tr_val_y\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "977e012e",
   "metadata": {},
   "source": [
    "# 6. Modeling - ML, XGB, LGBM\n",
    "- shop_type_big_label(0~4), shop_type_small_label(0~6), geo, average_sale_price\n",
    "    - input feature : 14\n",
    "- Multi-Class Classification \n",
    "- XGB\n",
    "- LGBM"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "705a33ef",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ml용 data에는 _ml을 붙여줍니다 \n",
    "X_column_ml = ['average_sale_price', 'shop_type_big_0',\n",
    "       'shop_type_big_1', 'shop_type_big_2', 'shop_type_big_3',\n",
    "       'shop_type_big_4', 'shop_type_small_0', 'shop_type_small_1',\n",
    "       'shop_type_small_2', 'shop_type_small_3', 'shop_type_small_4',\n",
    "       'shop_type_small_5', 'shop_type_small_6', 'geo']\n",
    "\n",
    "train_X_ml = train_X[X_column_ml].copy()\n",
    "valid_X_ml = valid_X[X_column_ml].copy()\n",
    "test_X_ml = test_X[X_column_ml].copy()\n",
    "\n",
    "print(len(train_X))\n",
    "print(len(train_X_ml))\n",
    "print(len(valid_X))\n",
    "print(len(valid_X_ml))\n",
    "print(len(test_X))\n",
    "print(len(test_X_ml))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bca7d026",
   "metadata": {},
   "outputs": [],
   "source": [
    "from lightgbm import LGBMClassifier\n",
    "from lightgbm import plot_importance \n",
    "from xgboost import XGBClassifier\n",
    "from xgboost import plot_importance\n",
    "from keras.callbacks import EarlyStopping, ModelCheckpoint\n",
    "# Learning task parameters\n",
    "# multi:softmax : softmax를 이용한 다중 클래스 분류 \n",
    "# multi:softptob : softmax를 이용한 다중 클래스에 대한 예상 확률 반환 \n",
    "# mlogloss : multiclass logloss \n",
    "\n",
    "model_xgb = XGBClassifier(\n",
    "    eval_metric='mlogloss', use_label_encoder=False)\n",
    "model_lgbm = LGBMClassifier(\n",
    "    eval_metric='mlogloss')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "61858132",
   "metadata": {},
   "outputs": [],
   "source": [
    "model_xgb.fit(train_X_ml, train_y, eval_set=[(valid_X_ml, valid_y)])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4811955d",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "model_lgbm.fit(train_X_ml, train_y, eval_set=[(valid_X_ml, valid_y)])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c0bd44f5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ML prediction\n",
    "pred_xgb_prob = model_xgb.predict_proba(test_X_ml)\n",
    "pred_xgb = np.argmax(pred_xgb_prob, axis=1)\n",
    "\n",
    "pred_lgbm_prob = model_lgbm.predict_proba(test_X_ml)\n",
    "pred_lgbm = np.argmax(pred_lgbm_prob, axis=1)\n",
    "\n",
    "pred_xgb"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "826e4c74",
   "metadata": {},
   "source": [
    "## 7. Modeling - DL, NLP, shop_name\n",
    "word embedding and embedding vector\n",
    "\n",
    "1. make vocabulary set (tk)\n",
    "2. using vocab set -> index encoding (seq_data)\n",
    "3. padding with 0 -> pad_seq_data\n",
    "- vocabulary set (tk)\n",
    "    - shop_name : 116,918 개의 단어 set\n",
    "    - shop_name + shop_type_big + shop_type_small : 114,967 개의 단어 set\n",
    "- nlp input length -> 13\n",
    "    - shop_name : 8\n",
    "    - shop_type_big : 3\n",
    "    - shop_type_small : 5\n",
    "    - text_concat : 15\n",
    "- shop_name : 0.75 acc\n",
    "- shop_name + shop_type_big + shop_type_small : 0.80\n",
    "- shop_name + shop_type_big + shop_type_small + geo : 0.80"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9d4ac813",
   "metadata": {},
   "outputs": [],
   "source": [
    "input_data['concat_text']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "af3b0a12",
   "metadata": {},
   "outputs": [],
   "source": [
    "from keras.preprocessing.text import Tokenizer\n",
    "\n",
    "tk = Tokenizer()\n",
    "tk.fit_on_texts(input_data['concat_text'])\n",
    "\n",
    "print(list(tk.word_index.items())[:20])\n",
    "print(\"\\nvocab words 개수 : \", len(tk.word_index.items()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bd57e06d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# test for nlp input length \n",
    "# shop_name, shop_type_big, shop_type_small, text_concat 중 가장  긴 input length 찾아야 함 \n",
    "from keras.preprocessing.sequence import pad_sequences\n",
    "\n",
    "seq_data = tk.texts_to_sequences(input_data['concat_text'])\n",
    "print(\"seq_data[0]: \", seq_data[0])\n",
    "\n",
    "pad_seq_data = pad_sequences(seq_data)\n",
    "print(\"pad_seq_data.shpae: \", pad_seq_data.shape)\n",
    "\n",
    "nlp_input_length = pad_seq_data[0].shape[0]\n",
    "print(\"nlp_input_length\", nlp_input_length)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1e3a814d",
   "metadata": {},
   "outputs": [],
   "source": [
    "def word_embedding(df, nlp_input_length) :\n",
    "    seq_data = tk.texts_to_sequences(df)\n",
    "    pad_seq_data = pad_sequences(seq_data, nlp_input_length)\n",
    "    word_embedding = pad_seq_data\n",
    "    return word_embedding\n",
    "\n",
    "train_X_dl = word_embedding(train_X['concat_text'], nlp_input_length)\n",
    "valid_X_dl = word_embedding(valid_X['concat_text'], nlp_input_length)\n",
    "test_X_dl = word_embedding(test_X['concat_text'], nlp_input_length)\n",
    "\n",
    "print(len(train_X))\n",
    "print(len(train_X_dl))\n",
    "print(len(valid_X))\n",
    "print(len(valid_X_dl))\n",
    "print(len(test_X))\n",
    "print(len(test_X_dl))   "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2fc47032",
   "metadata": {},
   "outputs": [],
   "source": [
    "from keras import Sequential\n",
    "from keras.layers import *\n",
    "import tensorflow as tf\n",
    "import keras\n",
    "from keras.callbacks import EarlyStopping, ModelCheckpoint, ReduceLROnPlateau\n",
    "\n",
    "def create_model(input_dim, output_dim, input_length) : \n",
    "    model = Sequential()\n",
    "    # 1 워드 임베딩 학습 \n",
    "    model.add(Embedding(input_dim, output_dim, input_length = input_length))\n",
    "    \n",
    "    # Classification 학습 \n",
    "    model.add(Flatten())\n",
    "    model.add(Dense(64, activation='relu'))\n",
    "    model.add(Dense(len(set(input_data_y)), activation='softmax'))\n",
    "    \n",
    "    model.compile(loss='sparse_categorical_crossentropy', optimizer='adam', metrics=['accuracy'])\n",
    "    return model\n",
    "\n",
    "\n",
    "input_dim = len(tk.word_index) + 1 # vocab size + padding index =114,537 + 1 = 114,538\n",
    "output_dim = 10\n",
    "input_length = nlp_input_length\n",
    "\n",
    "model = create_model(input_dim, output_dim, input_length)\n",
    "model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c6a9ab8a",
   "metadata": {},
   "outputs": [],
   "source": [
    "hist = model.fit(train_X_dl, train_y, validation_data=(valid_X_dl, valid_y), epochs=10, batch_size=64)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e7ab13ef",
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.plot(hist.history['loss'], label='loss')\n",
    "plt.plot(hist.history['val_loss'], label='test loss')\n",
    "plt.xticks(range(len(hist.history['loss'])))\n",
    "plt.xlabel('epoch')\n",
    "plt.ylabel('precision')\n",
    "plt.legend(loc='lower right')\n",
    "plt.grid(True)\n",
    "plt.show()\n",
    "# valid loss가 올라가는 순간 train data에 overfitting \n",
    "# 10 epochs로 통일 -> 10 epoch 이상은 시간도 오래 걸리고 변화가 크지 않았음 "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b9e41709",
   "metadata": {},
   "outputs": [],
   "source": [
    "pred_dl_prob = model.predict(test_X_dl)\n",
    "pred_dl = np.argmax(pred_dl_prob, axis=1)\n",
    "\n",
    "pred_dl"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0201440a",
   "metadata": {},
   "source": [
    "# 7. Evaluation\n",
    "- precision(정밀도) : True -> True \n",
    "- Recall(재현율) : True인 것을 맞춘 비율 \n",
    "- F1 score : precision 과 recall의 조화평균"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4e53e2b3",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import accuracy_score, classification_report, confusion_matrix, f1_score\n",
    "\n",
    "def show_pred(test_y, pred) :\n",
    "    true_y = test_y.to_numpy()\n",
    "    true_y = np.ravel(true_y)\n",
    "    \n",
    "    df_result = pd.DataFrame(list(zip(true_y, pred)), columns=['true_y', 'prediction'])\n",
    "    return df_result\n",
    "    \n",
    "def show_prediction_error(test_y, pred) :\n",
    "    true_y = test_y.to_numpy()\n",
    "    true_y = np.ravel(true_y)\n",
    "    error = pred - true_y\n",
    "    plt.hist(error, bins=25)\n",
    "    plt.xlabel(\"Prediction Error\")\n",
    "    _ = plt.ylabel(\"Count\")\n",
    "    \n",
    "def feature_importance(model_xgb) : \n",
    "    %matplotlib inline\n",
    "    plt.rcParams['axes.unicode_minus'] = False\n",
    "    font_path = \"C:/Windows/Fonts/NGULIM.TTF\"\n",
    "    font = fm.FontProperties(fname=font_path).get_name()\n",
    "    rc('font', family=font)\n",
    "\n",
    "    fig, ax = plt.subplots(figsize=(10,12))\n",
    "    plot_importance(model_xgb, ax=ax)\n",
    "    \n",
    "def graph(pred, test_label) :\n",
    "    plt.figure(figsize=(16, 9))\n",
    "    plt.plot(test_label, label = 'actual')\n",
    "    plt.plot(pred, label = 'prediction')\n",
    "    plt.legend()\n",
    "    plt.show()\n",
    "\n",
    "def classification_report_csv(report) : \n",
    "    df = pd.DataFrame(report).transpose()\n",
    "    df.to_csv('classfication_report.csv', index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cad9c06d",
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "# DL model \n",
    "print(classification_report(test_y, pred_dl)) # word embedding "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "89def36c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ML model\n",
    "#report = classification_report(test_y, pred_xgb, output_dict=True)\n",
    "print(classification_report(test_y, pred_xgb))\n",
    "#classification_report_csv(report)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a244881c",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# ML model\n",
    "#report = classification_report(test_y, pred_lgbm, output_dict=True)\n",
    "print(classification_report(test_y, pred_lgbm))\n",
    "#classification_report_csv(report)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "756228fa",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "feature_series = pd.Series(data=model_xgb.feature_importances_, index=train_X.columns)\n",
    "feature_series = feature_series.sort_values(ascending=False) \n",
    "sns.barplot(x = feature_series, y=feature_series.index)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4aeb591d",
   "metadata": {},
   "source": [
    "# 8. Ensemble\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "42a60b13",
   "metadata": {},
   "outputs": [],
   "source": [
    "pred_ensemble_prob = pred_xgb_prob * 0.5 + pred_dl_prob * 0.5\n",
    "pred_ensemble = np.argmax(pred_ensemble_prob, axis=1)\n",
    "pred_ensemble"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3893d49b",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(classification_report(test_y, pred_ensemble))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ce4952c8",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
