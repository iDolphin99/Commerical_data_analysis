{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "23d7d438",
   "metadata": {},
   "source": [
    "# 1. Data Load"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f65e8299",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import math\n",
    "\n",
    "from scipy import stats #Analysis \n",
    "from scipy.stats import norm \n",
    "\n",
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt\n",
    "from matplotlib import rcParams\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "175a42b3",
   "metadata": {},
   "outputs": [],
   "source": [
    "path = \"./Dataset/\"\n",
    "\n",
    "data = pd.read_csv(path+\"kwproja_data_location.csv\")\n",
    "#data = pd.read_csv('../input/dolphin-kwproja-bigdata/kwproja_data_big.csv')\n",
    "\n",
    "# original data -> data\n",
    "data"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1eecf56c",
   "metadata": {},
   "source": [
    "# 2. EDA \n",
    "\n",
    "2,927,739 rows X 11 columns\n",
    "\n",
    "- 매장 속성 정보\n",
    "  - shop_code -> 식별자 feature => drop   \n",
    "  - shop_name => DL\n",
    "  - longtitude : 경도, latitude : 위도 -> 매장 위치 (회사 근처, 학교 근처 등 매출 영향성 있음) -> K-mean clustering => geo, ML, DL\n",
    "  - address1, address2 : GeoEncoder를 통해 따로 얻은 행정동, 1(30), 2(436) => DL\n",
    "  - shop_type_big -> 15 category, shop_type_big_label, ML => DL\n",
    "  - shop_type_small -> 61 category, shop_type_small_label, ML => DL \n",
    "\n",
    "- 매출 정보\n",
    "  - date -> 24 category, 201606~ 201805 까지의 data\n",
    "  - monthly_gain / avearge_sale_price = 한달 총 판매수\n",
    "\n",
    "- 매출 통계 정보-> X"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8160023d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# rename data columns and check the data\n",
    "data.columns = ['date', 'shop_code', 'shop_name', 'shop_type_big', 'shop_type_small', \n",
    "                'longitude', 'latitude', 'monthly_gain', 'average_sale_price', 'address1', 'address2']\n",
    "\n",
    "print(data.columns, '\\n')\n",
    "print(data.info())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3f6c9722",
   "metadata": {},
   "source": [
    "# 3. Preprocessing\n",
    "data -> processed_data\n",
    "- 9 columns : **shop_code | date | shop_name | shop_type_big | shop_type_small | longitude | latitude | average_sale_price | monthly_gain**\n",
    "- shop_code : 식별자 feature 이므로 drop \n",
    "- date : 아직은 쓸 수 없으므로 drop\n",
    "- missing value 제거 : A/N\n",
    "- shop_type_big(15), shop_type_small(61) \n",
    "    - ML : label encodding\n",
    "    - DL : NLP\n",
    "- longitude, latitude : \n",
    "    - ML : k-mean clustering -> geo column \n",
    "    - DL : NLP, reverse geo encoder(행정동, 법정동, 지번주소, 도로명주소) -> 지번주소 가져오세요(for web) \n",
    "    - 행정동admcode, 법정동legalcode -> area1, area2, area3, area4\n",
    "    - 지번 주소addr -> area1, area2, area3, area4 (x), land -> namber1, number2\n",
    "    - 도로명 주소roadaddr -> area1, area2, area3, area4(x), land -> number1, number2, name  \n",
    "- average_sale_price \n",
    "    - log transformation \n",
    "- MinMaxSaclar 정규화 -> 정규화 column의 범위는?? 실험필요 요인 "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "746b4c66",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 원본 data와 따로 관리 -> original data = data, preprocessed data = processed_data \n",
    "# feature drop : date, shop_code\n",
    "processed_data = data.drop(['date', 'shop_code'], axis=1).copy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a9ad35fa",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# There are no missing values \n",
    "# missing value drop - monthly_gain\n",
    "null_index = processed_data[processed_data['monthly_gain']==0].index \n",
    "print(\"monthly gain null : \", len(null_index))\n",
    "processed_data = processed_data.drop(null_index)\n",
    "\n",
    "# missing value drop - average_sale_price\n",
    "null_index = processed_data[processed_data['average_sale_price']==0].index \n",
    "print(\"average sale price null : \", len(null_index))\n",
    "processed_data = processed_data.drop(null_index)\n",
    "\n",
    "# missing value drop - shop_type_big\n",
    "null_index = processed_data[processed_data['shop_type_big'].isnull()==True].index\n",
    "print(\"shop type big null : \", len(null_index))\n",
    "print(\"shop type big unique : \", processed_data['shop_type_big'].nunique())\n",
    "processed_data = processed_data.drop(null_index)\n",
    "\n",
    "# missing value drop - shop_type_small \n",
    "null_index = processed_data[processed_data['shop_type_small'].isnull()==True].index\n",
    "print(\"shop type small null : \", len(null_index))\n",
    "print(\"shop type small unique : \", processed_data['shop_type_small'].nunique() )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bb83a47b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# LabelEncoder\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "\n",
    "# fit transform으로 한번에 처리 가능\n",
    "le = LabelEncoder()\n",
    "processed_data['shop_type_big_label'] = le.fit_transform(list(processed_data['shop_type_big']))   \n",
    "print(le.classes_)\n",
    "\n",
    "le = LabelEncoder()\n",
    "processed_data['shop_type_small_label'] = le.fit_transform(list(processed_data['shop_type_small'])) \n",
    "print(le.classes_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e08985e0",
   "metadata": {},
   "outputs": [],
   "source": [
    "# BinaryEncoder for categorical variable \n",
    "import category_encoders as ce\n",
    "\n",
    "encoder = ce.BinaryEncoder(cols=[\"shop_type_big\", \"shop_type_small\"])\n",
    "df = encoder.fit_transform(processed_data[[\"shop_type_big\", \"shop_type_small\"]])\n",
    "\n",
    "processed_data = pd.concat([processed_data, df], axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "783a0995",
   "metadata": {},
   "outputs": [],
   "source": [
    "# NLP용 preprocessing \n",
    "# shop_name, shop_type_big, shop_type_small = concat_text \n",
    "processed_data['text'] = processed_data['shop_name'] + ' ' + processed_data['shop_type_big'] + ' ' + processed_data['shop_type_small']\n",
    "processed_data['address'] = processed_data['address1'] + ' ' + processed_data['address2']\n",
    "processed_data['concat_text'] = processed_data['text'] + ' ' + processed_data['address']\n",
    "processed_data = processed_data.drop(['text', 'shop_name', 'shop_type_big', 'shop_type_small', 'address', 'address1', 'address2'], axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "236323b6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Labeling - KMeans Clustering \n",
    "# longitude + latitude = geo \n",
    "from sklearn.cluster import KMeans\n",
    "\n",
    "kmeans = KMeans(n_clusters=9).fit(processed_data[['latitude', 'longitude']])\n",
    "print(kmeans.cluster_centers_)\n",
    "print(kmeans.labels_)\n",
    "\n",
    "processed_data['geo'] = kmeans.labels_\n",
    "\n",
    "# plotting geo\n",
    "sns.scatterplot(x='longitude' , y='latitude', hue=\"geo\", data=processed_data, palette=\"Paired\")\n",
    "plt.title('k-mean')\n",
    "\n",
    "processed_data = processed_data.drop(['longitude', 'latitude'], axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "de36c722",
   "metadata": {},
   "outputs": [],
   "source": [
    "# average_sale_price -> skewed data \n",
    "# log transfromation \n",
    "from sklearn.preprocessing import MinMaxScaler, RobustScaler, StandardScaler, Normalizer\n",
    "\n",
    "scale_cols = ['average_sale_price']\n",
    "processed_data[scale_cols] = processed_data[scale_cols].apply(lambda x : np.log1p(x))\n",
    "\n",
    "processed_data"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "61d58240",
   "metadata": {},
   "source": [
    "# 4. Target Variable Labeling and EDA for variable y\n",
    "processed_data -> labeled_data \n",
    "- 어떻게 라벨링 할 것인가? \n",
    "- 1. Classification : use mean, std -> failed\n",
    "- 2. Classification : Quantile 10%, 20%, 25%, 33% -> label 10, 5, 4, 3 \n",
    "- 3. Classification : Quantile by shop_type_big with lower fence, Q2, upper_fence -> label 31\n",
    "- 3. **Classification : Quantile by shop_type_big with Q1, Q2, Q3 -> label 45** \n",
    "- 4. Classification : price label, min:5, max:181억 -> label 15\n",
    "- 5. Removing Outlier : outler 233,140 -> total data(without outlier) 2,694,599\n",
    "- 6. Rounding data "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9306e3c3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# pandas option 설정 하는 코드\n",
    "# monthly_gain의 경우 부동소수점으로 나타나서 보기 어려울땐 윗 줄의 주석을 제거하고 아래에 주석을 추가하고\n",
    "# 다시 원래대로 돌리고 싶다면 아래에 주석제거, 위 코드에 주석추가\n",
    "pd.options.display.float_format = '{:.2f}'.format\n",
    "#pd.reset_option('display.float_format')\n",
    "\n",
    "# 전처리된 data와 따로 관리 -> preprocessed data = procssed_data, labeled data = labeled_data \n",
    "labeled_data = processed_data.copy()\n",
    "labeled_data"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "76fedddb",
   "metadata": {},
   "source": [
    "#### Option) Removing outlier\n",
    "upper fence, lower fence 외 값(outlier)을 제거합니다 \n",
    "- 2,927,739 x 19   ->   2,694,599 x 19"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b635be51",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_pricelist(i, data) :\n",
    "    df = data[['shop_type_big_label','monthly_gain']].groupby('shop_type_big_label')\n",
    "    Q1 = df.get_group(i)['monthly_gain'].quantile(0.25)\n",
    "    Q2 = df.get_group(i)['monthly_gain'].quantile(0.5)\n",
    "    Q3 = df.get_group(i)['monthly_gain'].quantile(0.75)\n",
    "    IQR = Q3 - Q1\n",
    "    lower_fence = Q1 - (1.5 * IQR)\n",
    "    upper_fence = Q3 + (1.5 * IQR)\n",
    "    if lower_fence <= 0 : lower_fence = 0\n",
    "        \n",
    "    return lower_fence, Q1, Q2, Q3, upper_fence\n",
    "\n",
    "def remove_outlier(data, processed_data) :\n",
    "    output_data = data.copy()\n",
    "    for i in range(0,15) :\n",
    "        lower_fence, Q1, Q2, Q3, upper_fence = get_pricelist(i, processed_data)\n",
    "        shoptype_index = data[data.shop_type_big_label == i].index\n",
    "        shoptype_data = data.iloc[shoptype_index, :]\n",
    "        outlier_index = shoptype_data[shoptype_data.monthly_gain > upper_fence].index\n",
    "        print(\"removed index in shop_type_big\" , i, \": \", len(outlier_index))\n",
    "        output_data = output_data.drop(outlier_index)\n",
    "    return output_data \n",
    "\n",
    "labeled_data = remove_outlier(labeled_data, processed_data)\n",
    "labeled_data"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ccad9629",
   "metadata": {},
   "source": [
    "#### Option) Rounding data\n",
    "십만원대, 백만원대 아래 가격들은 모두 반올림 하여 비슷한 label 값을 가지는 것들은 통일\n",
    "- label의 개수를 줄임 45 -> 36\n",
    "- 1의 자리~ 10,000의 자리 숫자들은 반올림하에 0으로 처리"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b2072c43",
   "metadata": {},
   "outputs": [],
   "source": [
    "#print(\"original monthly_gain_label counts : \", labeled_data['gain_label'].nunique())\n",
    "#labeled_data['gain_label'] = labeled_data.gain_label.apply(lambda x : round(x, -5) if x < 10000000 else round(x, -6))\n",
    "\n",
    "#print(\"rounded monthly_gain_label value counts :\", labeled_data['gain_label'].nunique(), \n",
    "#      \"\\n\", labeled_data['gain_label'].value_counts())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "24cef785",
   "metadata": {},
   "source": [
    "# 5. Data Split \n",
    "전처리 완료, 필요한 column을 input으로 넣고 train / valid / test data split "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cc202a90",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split, cross_validate, StratifiedKFold, GridSearchCV\n",
    "\n",
    "# 전처리된 data와 따로 관리 -> labeled data = labeled_data, model input data = input_data\n",
    "input_data = labeled_data.copy()\n",
    "\n",
    "input_data_y = input_data['monthly_gain'].copy()\n",
    "input_data_X = input_data.drop(['monthly_gain'], axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "391ace80",
   "metadata": {},
   "outputs": [],
   "source": [
    "# train/ test data 로 split \n",
    "tr_val_X, test_X, tr_val_y, test_y = train_test_split(\n",
    "    input_data_X, \n",
    "    input_data_y, \n",
    "    test_size = 0.2, \n",
    "    random_state = 42,\n",
    "    shuffle=True\n",
    ")\n",
    "\n",
    "# valid/train 로 split\n",
    "train_X, valid_X, train_y, valid_y = train_test_split(\n",
    "    tr_val_X, \n",
    "    tr_val_y, \n",
    "    test_size = 0.2, \n",
    "    random_state = 42,\n",
    "    shuffle=True\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "55412fa2",
   "metadata": {},
   "source": [
    "# 6. Modeling - ML, XGB, LGBM\n",
    "- shop_type_big_label(0~4), shop_type_small_label(0~6), geo, average_sale_price\n",
    "    - input feature : 14\n",
    "- XGB\n",
    "- LGBM \n",
    "    - tweedie?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b308060a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ml용 data에는 _ml을 붙여줍니다 \n",
    "X_column_ml = ['average_sale_price', 'shop_type_big_0',\n",
    "       'shop_type_big_1', 'shop_type_big_2', 'shop_type_big_3',\n",
    "       'shop_type_big_4', 'shop_type_small_0', 'shop_type_small_1',\n",
    "       'shop_type_small_2', 'shop_type_small_3', 'shop_type_small_4',\n",
    "       'shop_type_small_5', 'shop_type_small_6', 'geo']\n",
    "\n",
    "train_X_ml = train_X[X_column_ml].copy()\n",
    "valid_X_ml = valid_X[X_column_ml].copy()\n",
    "test_X_ml = test_X[X_column_ml].copy()\n",
    "\n",
    "print(len(train_X))\n",
    "print(len(train_X_ml))\n",
    "print(len(valid_X))\n",
    "print(len(valid_X_ml))\n",
    "print(len(test_X))\n",
    "print(len(test_X_ml))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f42ec416",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import mean_squared_error, r2_score, mean_absolute_error\n",
    "from keras.callbacks import EarlyStopping, ModelCheckpoint\n",
    "\n",
    "from lightgbm import LGBMRegressor\n",
    "from lightgbm import plot_importance \n",
    "from xgboost import XGBRegressor\n",
    "from xgboost import plot_importance\n",
    "from sklearn.linear_model import Ridge,Lasso\n",
    "\n",
    "model_xgb = XGBRegressor()\n",
    "model_lgbm = LGBMRegressor()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "77bfb68c",
   "metadata": {},
   "outputs": [],
   "source": [
    "model_xgb.fit(train_X_ml,train_y,eval_set=[(valid_X_ml,valid_y)])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "484fd0c7",
   "metadata": {},
   "outputs": [],
   "source": [
    "model_lgbm.fit(train_X_ml,train_y,eval_set=[(valid_X_ml,valid_y)])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9c237da0",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ML prediction\n",
    "pred_xgb = model_xgb.predict(test_X_ml)\n",
    "\n",
    "pred_lgbm = model_lgbm.predict(test_X_ml)\n",
    "pred_lgbm"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b6703f05",
   "metadata": {},
   "source": [
    "#### Ridge"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d0eaa90d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# RIdge 모델 (parameter 적용)\n",
    "from sklearn.linear_model import RidgeCV #parameter를 넣어준다는거에서 ridge랑 다름\n",
    "\n",
    "alphas = [0, 0.001, 0.01, 0.1, 1]\n",
    "\n",
    "# RidgeCV는 alpha로 넣고자 하는 값들을 리스트로 전달하면 내부적으로 최적의 alpha값을 찾아냄\n",
    "ridgecv = RidgeCV(alphas=alphas, normalize=True, cv=5)\n",
    "# cv : cross-validation -> 데이터를 k등분한 후 각각에 대하여 검증 진행\n",
    "# 검증 결과 가장 점수가 높은 모델을 채택\n",
    "ridgecv.fit(train_X_ml, train_y)\n",
    "ridgecv_pred = ridgecv.predict(test_X_ml)\n",
    "\n",
    "mae = mean_absolute_error(test_y, ridgecv_pred)\n",
    "r2 = r2_score(test_y, ridgecv_pred)\n",
    "print(f'Test MAE: ${mae:,.0f}')\n",
    "print(f'R2 Score: {r2:,.4f}\\n')\n",
    "\n",
    "print(f'alpha: {ridgecv.alpha_}') # 최종 결정된 alpha값\n",
    "print(f'cv best score: {ridgecv.best_score_}') # 최종 alpha에서의 점수(R^2 of self.predict(X) wrt. y.)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4e51f2aa",
   "metadata": {},
   "outputs": [],
   "source": [
    "#위의 alpha값 넣어준 후 학습 진행하기\n",
    "model_ridge=Ridge(alpha=0.01)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7e91f4bb",
   "metadata": {},
   "outputs": [],
   "source": [
    "model_ridge.fit(train_X_ml,train_y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2d361b3f",
   "metadata": {},
   "outputs": [],
   "source": [
    "pred_ridge = model_ridge.predict(test_X_ml)\n",
    "pred_ridge"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "84c35933",
   "metadata": {},
   "source": [
    "#### Lasso"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ce7fe710",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Parameter 튜닝시도\n",
    "model_lasso=Lasso()\n",
    "model_lasso.fit(train_X_ml,train_y)\n",
    "print(model_lasso.score(train_X_ml,train_y))\n",
    "print(model_lasso.score(test_X_ml,test_y))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dd587d6c",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_score=[]\n",
    "test_score=[]\n",
    "alpha_list=[0.001,0.01,0.1,1,10,100]\n",
    "for alpha in alpha_list:\n",
    "    model_lasso=Lasso(alpha=alpha,max_iter=10000)\n",
    "    model_lasso.fit(train_X_ml,train_y)\n",
    "    train_score.append(model_lasso.score(train_X_ml,train_y))\n",
    "    test_score.append(model_lasso.score(test_X_ml,test_y))\n",
    "plt.plot(np.log10(alpha_list),train_score)\n",
    "plt.plot(np.log10(alpha_list),test_score)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2238f9cf",
   "metadata": {},
   "outputs": [],
   "source": [
    "#test1\n",
    "lasso1= Lasso(alpha=0.01, max_iter=100000).fit(train_X_ml, train_y)\n",
    "\n",
    "print(\"훈련 세트의 정확도 : {:.2f}\".format(lasso1.score(train_X_ml, train_y)))\n",
    "\n",
    "print(\"테스트 세트의 정확도 : {:.2f}\".format(lasso1.score(test_X_ml, test_y)))\n",
    "\n",
    "print(\"사용한 특성의 수 : {}\".format(np.sum(lasso1.coef_ != 0)))\n",
    "\n",
    "print(\"사용한 max_iter : {}\".format(lasso1.n_iter_))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "41e3b410",
   "metadata": {},
   "outputs": [],
   "source": [
    "# --> 차이점이 보이지 않아 default값으로 구현\n",
    "model_lasso=Lasso()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7b7a9ca9",
   "metadata": {},
   "outputs": [],
   "source": [
    "model_lasso.fit(train_X_ml,train_y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "99a8c881",
   "metadata": {},
   "outputs": [],
   "source": [
    "pred_lasso = model_lasso.predict(test_X_ml)\n",
    "pred_lasso"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fbbc45c6",
   "metadata": {},
   "source": [
    "## 7. Modeling - DL, NLP, shop_name\n",
    "word embedding and embedding vector\n",
    "\n",
    "1. make vocabulary set (tk)\n",
    "2. using vocab set -> index encoding (seq_data)\n",
    "3. padding with 0 -> pad_seq_data\n",
    "- vocabulary set (tk)\n",
    "    - shop_name : 116,918 개의 단어 set\n",
    "    - shop_name + shop_type_big + shop_type_small : 114,967 개의 단어 set\n",
    "- nlp input length -> 13\n",
    "    - shop_name : 8\n",
    "    - shop_type_big : 3\n",
    "    - shop_type_small : 5\n",
    "    - text_concat : 15\n",
    "- shop_name : 0.75 acc\n",
    "- shop_name + shop_type_big + shop_type_small : 0.80\n",
    "- shop_name + shop_type_big + shop_type_small + geo : 0.80"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2eff8d46",
   "metadata": {},
   "outputs": [],
   "source": [
    "from keras.preprocessing.text import Tokenizer\n",
    "\n",
    "tk = Tokenizer()\n",
    "tk.fit_on_texts(input_data['concat_text'])\n",
    "\n",
    "print(list(tk.word_index.items())[:20])\n",
    "print(\"\\nvocab words 개수 : \", len(tk.word_index.items()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bfeed9ef",
   "metadata": {},
   "outputs": [],
   "source": [
    "from keras.preprocessing.sequence import pad_sequences\n",
    "\n",
    "seq_data = tk.texts_to_sequences(input_data['concat_text'])\n",
    "print(\"seq_data[0]: \", seq_data[0])\n",
    "\n",
    "pad_seq_data = pad_sequences(seq_data)\n",
    "print(\"pad_seq_data.shpae: \", pad_seq_data.shape)\n",
    "\n",
    "nlp_input_length = pad_seq_data[0].shape[0]\n",
    "print(\"nlp_input_length\", nlp_input_length)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1432f1a1",
   "metadata": {},
   "outputs": [],
   "source": [
    "def word_embedding(df, nlp_input_length) :\n",
    "    seq_data = tk.texts_to_sequences(df)\n",
    "    pad_seq_data = pad_sequences(seq_data, nlp_input_length)\n",
    "    word_embedding = pad_seq_data\n",
    "    return word_embedding\n",
    "\n",
    "train_X_dl = word_embedding(train_X['concat_text'], nlp_input_length)\n",
    "valid_X_dl = word_embedding(valid_X['concat_text'], nlp_input_length)\n",
    "test_X_dl = word_embedding(test_X['concat_text'], nlp_input_length)\n",
    "\n",
    "print(len(train_X))\n",
    "print(len(train_X_dl))\n",
    "print(len(valid_X))\n",
    "print(len(valid_X_dl))\n",
    "print(len(test_X))\n",
    "print(len(test_X_dl))   "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2598bd79",
   "metadata": {},
   "outputs": [],
   "source": [
    "from keras import Sequential\n",
    "from keras.layers import *\n",
    "import tensorflow as tf\n",
    "import keras\n",
    "from keras.callbacks import EarlyStopping, ModelCheckpoint, ReduceLROnPlateau\n",
    "\n",
    "def create_model(input_dim, output_dim, input_length=nlp_input_length) : \n",
    "    model = Sequential()\n",
    "    model.add(Embedding(input_dim, output_dim, input_length = nlp_input_length))    \n",
    "    model.add(Flatten())\n",
    "    model.add(Dense(1, activation='relu'))\n",
    "    model.add(Dense(len(set(input_data_y)), activation='linear'))\n",
    "    \n",
    "    model.compile(loss='mean_squared_error', optimizer='adam', metrics=['MeanSquaredError'])\n",
    "    return model\n",
    "\n",
    "input_dim = len(tk.word_index) + 1\n",
    "output_dim = 10\n",
    "\n",
    "model_dl = create_model(input_dim, output_dim)\n",
    "model_dl.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d773a0cd",
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "hist = model_dl.fit(train_X_dl, train_y, validation_data=(valid_X_dl, valid_y), epochs=3, batch_size=64)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0ce317d7",
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.plot(hist.history['loss'], label='loss')\n",
    "plt.plot(hist.history['val_loss'], label='test loss')\n",
    "plt.xticks(range(len(hist.history['loss'])))\n",
    "plt.xlabel('epoch')\n",
    "plt.ylabel('precision')\n",
    "plt.legend(loc='lower right')\n",
    "plt.grid(True)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9bf6af8a",
   "metadata": {},
   "outputs": [],
   "source": [
    "pred_dl= model_dl.predict(test_X_dl)\n",
    "pred_dl"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c68d1a4e",
   "metadata": {},
   "source": [
    "## 7. Evaluation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "063e0b5e",
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_history(history):\n",
    "    #hist = pd.DataFrame(history.history)\n",
    "    #history['epoch'] = history.epoch\n",
    "    \n",
    "    plt.figure(figsize=(8,12))\n",
    "    \n",
    "    plt.subplot(2,1,1)\n",
    "    plt.xlabel('Epoch')\n",
    "    plt.ylabel('Mean Abs Error')\n",
    "    plt.plot(hist['epoch'], hist['mae'],\n",
    "           label='Train Error')\n",
    "    plt.plot(hist['epoch'], hist['val_mae'],\n",
    "           label = 'Val Error')\n",
    "    plt.legend()\n",
    "    \n",
    "    plt.subplot(2,1,2)\n",
    "    plt.xlabel('Epoch')\n",
    "    plt.ylabel('Mean Square Error')\n",
    "    plt.plot(hist['epoch'], hist['mse'],\n",
    "           label='Train Error')\n",
    "    plt.plot(hist['epoch'], hist['val_mse'],\n",
    "           label = 'Val Error')\n",
    "    plt.legend()\n",
    "    plt.show()\n",
    "    \n",
    "def show_pred(test_y, pred) :\n",
    "    true_y = test_y.to_numpy()\n",
    "    true_y = np.ravel(true_y)\n",
    "    \n",
    "    df_result = pd.DataFrame(list(zip(true_y, pred)), columns=['true_y', 'prediction'])\n",
    "    return df_result\n",
    "\n",
    "def show_mse_rmse(test_y, pred) :\n",
    "    mse = mean_squared_error(test_y, pred)\n",
    "    print(\"mse : %f\" % mse)\n",
    "    \n",
    "    rmse = np.sqrt(mse)\n",
    "    print(\"rmse: %f \\n\" %rmse)\n",
    "\n",
    "def show_r2_score(test_y, pred, test_X) : \n",
    "    r2 = r2_score(pred, test_y)\n",
    "    print(\"r2 : %f \" % r2)\n",
    "    adj_r2 = 1-(1-r2_score(test_y, pred)) * (len(test_y)-1) / (len(test_y) - test_X.shape[1] - 1)\n",
    "    print(\"adj_r2_score : %f \\n\" % adj_r2)\n",
    "    \n",
    "def show_prediction_error(test_y, pred) :\n",
    "    true_y = test_y.to_numpy()\n",
    "    true_y = np.ravel(true_y)\n",
    "    error = pred - true_y\n",
    "    plt.hist(error, bins=25)\n",
    "    plt.xlabel(\"Prediction Error\")\n",
    "    _ = plt.ylabel(\"Count\")\n",
    "    \n",
    "def feature_importance(model_xgb) : \n",
    "    %matplotlib inline\n",
    "    plt.rcParams['axes.unicode_minus'] = False\n",
    "    font_path = \"C:/Windows/Fonts/NGULIM.TTF\"\n",
    "    font = fm.FontProperties(fname=font_path).get_name()\n",
    "    rc('font', family=font)\n",
    "\n",
    "    fig, ax = plt.subplots(figsize=(10,12))\n",
    "    plot_importance(model_xgb, ax=ax)\n",
    "    \n",
    "def graph(pred, test_label) :\n",
    "    plt.figure(figsize=(16, 9))\n",
    "    plt.plot(test_label, label = 'actual')\n",
    "    plt.plot(pred, label = 'prediction')\n",
    "    plt.legend()\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "572dffd7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ML model \n",
    "# XGB\n",
    "show_pred(test_y, pred_xgb)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4d8faabb",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ML model \n",
    "# XGB, LGBM, Ridge, Lasso \n",
    "show_mse_rmse(test_y, pred_xgb)\n",
    "show_r2_score(test_y, pred_xgb, test_X_ml)\n",
    "\n",
    "print()\n",
    "show_mse_rmse(test_y, pred_lgbm)\n",
    "show_r2_score(test_y, pred_lgbm, test_X_ml)\n",
    "\n",
    "print()\n",
    "show_mse_rmse(test_y, pred_ridge)\n",
    "show_r2_score(test_y, pred_ridge, test_X_ml)\n",
    "\n",
    "print()\n",
    "show_mse_rmse(test_y, pred_lasso)\n",
    "show_r2_score(test_y, pred_lasso, test_X_ml)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ba1d5e82",
   "metadata": {},
   "outputs": [],
   "source": [
    "# DL model \n",
    "show_pred(test_y, pred_dl)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "91b84402",
   "metadata": {},
   "outputs": [],
   "source": [
    "show_mse_rmse(test_y, pred_dl)\n",
    "show_r2_score(test_y, pred_dl, test_X_dl)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
